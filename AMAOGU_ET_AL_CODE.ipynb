{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AGMX5K5AbazB"},"outputs":[],"source":"\n#SECTION A: Greening and Browning Mapping\n\n#  Always import cartopy before geemap to avoid widget conflicts\nimport ee\nimport geemap, ipyleaflet\nimport matplotlib.pyplot as plt\nfrom geemap import cartoee\n\nprint(\"All required libraries (cartopy, geemap, ee, matplotlib) loaded successfully!\")\n\nee.Authenticate()\n\nee.Initialize(project='danielsiglab')\n\n\n\n"},{"cell_type":"code","source":"\nimport ee\n\n# 1. Load All Tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\") (splitting our study area in tiles)\n\n# 2. Load Full Ecozone Layer\n\nfull_ecozone_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/studyarea50\")\n\n# 2. Rasterize Ecozone Field\n#ecozone_raster = full_ecozone_fc.reduceToImage(\n  #  properties=['ECOZONE_ID'],\n   # reducer=ee.Reducer.first()\n#).rename('Ecozone\n\necozone_raster = full_ecozone_fc.reduceToImage(\n  properties=['ID'],\n   reducer=ee.Reducer.first()\n).rename('Ecozone')\n\n\nslope_green_image = ecozone_raster.remap(\n    [3,4,5,6,9,12,14,15,16],\n    [0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015 ]\n).rename('SlopeGreen')\n\nslope_brown_image = ecozone_raster.remap(\n    [3,4,5,6,9,12,14,15,16],\n    [-0.0015, -0.0015, -0.0015, -0.0015, -0.0015, -0.0015, -0.0015, -0.0015, -0.0015]\n).rename('SlopeBrown')\n\n\n# Convert Tile Collection to List\ntile_list = tiles_fc.toList(tiles_fc.size())\n\n# 6. Define Number of Tiles\nn_tiles = tile_list.length()\n\n# 7. Define Analysis Period\nstart_year = 1986\nend_year = 2024\nyear_list = ee.List.sequence(start_year, end_year)\n\n#  8. Tile Processing Function\ndef process_tile(tile_index):\n    tile_index = ee.Number(tile_index)\n    tile_feature = ee.Feature(tile_list.get(tile_index))\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    #  8.1 Load Ecozones for this tile\n    tile_ecozones = full_ecozone_fc.filterBounds(tile_geom)\n\n    # 8.2 Rasterize Ecozone ID for this tile\n    tile_ecozone_raster = tile_ecozones.reduceToImage(\n        properties=['ID'],\n        reducer=ee.Reducer.first()\n    ).rename('Ecozone').clip(tile_geom)\n\n    # 8.3 Build Per-Year Fire Memory Dictionary for This Tile\n    fire_memory_dict = ee.Dictionary.fromLists(\n        year_list.map(lambda year: ee.String(year)),\n        year_list.map(lambda year: ee.ImageCollection([\n            fire_raster.eq(ee.Number(year).subtract(offset)).selfMask()\n            for offset in range(11)\n        ]).max().clip(tile_geom))\n    )\n\n    return fire_memory_dict  # Optional, to confirm structure\n\n# Confirm function output (optional, triggers server-side eval)\n#print(process_tile(0).getInfo())\n","metadata":{"id":"x7XJpCURbtY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"-dWzBZ5RcfEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 4. Load Fire Raster\nfire_raster = ee.Image(\"projects/sat-io/open-datasets/CA_FOREST/NBAC/NBAC_MRB_1972_to_2023\")\n\n# 5â€“7. Tile and Year Setup\ntile_list = tiles_fc.toList(tiles_fc.size())\nn_tiles = tile_list.length()\nstart_year = 1986\nend_year = 2024\nyear_list = ee.List.sequence(start_year, end_year)\n\n# 8. Process Each Tile\ndef process_tile(tile_index):\n    tile_index = ee.Number(tile_index)\n    tile_feature = ee.Feature(tile_list.get(tile_index))\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    tile_ecozone_raster = full_ecozone_fc.filterBounds(tile_geom).reduceToImage(\n        ['ID'], ee.Reducer.first()\n    ).rename('Ecozone').clip(tile_geom)\n\n    def fire_memory(year):\n        return ee.ImageCollection([fire_raster.eq(ee.Number(year).subtract(offset)).selfMask()\n                                   for offset in range(11)]).max().clip(tile_geom)\n\n    fire_memory_dict = ee.Dictionary.fromLists(\n        year_list.map(lambda y: ee.String(y)),\n        year_list.map(lambda y: fire_memory(y))\n    )\n\n    def load_and_score_landsat(year):\n        start = ee.Date.fromYMD(year, 7, 1)\n        end = ee.Date.fromYMD(year, 9, 30)\n\n        def select_bands(col, bands_in, bands_out):\n            return col.filterBounds(tile_geom).filterDate(start, end).select(bands_in, bands_out)\n\n        l5 = select_bands(ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'),\n                          ['SR_B1','SR_B3','SR_B4','SR_B5','SR_B7','QA_PIXEL','SR_ATMOS_OPACITY'],\n                          ['BLUE','RED','NIR','SWIR1','SWIR2','QA','AERO'])\n\n        l7 = select_bands(ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'),\n                          ['SR_B1','SR_B3','SR_B4','SR_B5','SR_B7','QA_PIXEL','SR_ATMOS_OPACITY'],\n                          ['BLUE','RED','NIR','SWIR1','SWIR2','QA','AERO'])\n\n        l8 = select_bands(ee.ImageCollection('LANDSAT/LC08/C02/T1_L2'),\n                          ['SR_B2','SR_B4','SR_B5','SR_B6','SR_B7','QA_PIXEL','SR_QA_AEROSOL'],\n                          ['BLUE','RED','NIR','SWIR1','SWIR2','QA','AERO'])\n\n        l9 = select_bands(ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'),\n                          ['SR_B2','SR_B4','SR_B5','SR_B6','SR_B7','QA_PIXEL','SR_QA_AEROSOL'],\n                          ['BLUE','RED','NIR','SWIR1','SWIR2','QA','AERO'])\n\n        collection = l5.merge(l7).merge(l8).merge(l9)\n\n        def score_image(img):\n            ndvi = img.normalizedDifference(['NIR', 'RED']).rename('NDVI')\n            evi = img.expression('2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',\n                                 {'NIR': img.select('NIR'), 'RED': img.select('RED'), 'BLUE': img.select('BLUE')}).rename('EVI')\n            nbr = img.normalizedDifference(['NIR', 'SWIR2']).rename('NBR')\n\n            spacecraft = ee.String(img.get('SPACECRAFT_ID'))\n            isL5 = spacecraft.compareTo(ee.String('LANDSAT_5')).eq(0)\n            isL7 = spacecraft.compareTo(ee.String('LANDSAT_7')).eq(0)\n            isL57 = isL5.Or(isL7)\n            date = ee.Date(img.get('DATE_ACQUIRED'))\n            isL7After2003 = isL7.And(date.millis().gt(ee.Date('2003-06-01').millis()))\n            sensor_score = ee.Image.constant(1).where(isL7After2003, 0.5)\n            slc_penalty = ee.Image.constant(1).where(isL7After2003, 0.8)\n\n            cloud = img.select('QA').bitwiseAnd(1 << 3).neq(0)\n            shadow = img.select('QA').bitwiseAnd(1 << 4).neq(0)\n            cloud_shadow = cloud.Or(shadow)\n            dist = cloud_shadow.updateMask(cloud_shadow).fastDistanceTransform(256).sqrt()\n            dreq = ee.Number(50)\n            dmid = dreq.divide(2)\n            dist_score = ee.Image(1).divide(ee.Image(1).add(dist.min(dreq).subtract(dmid).multiply(-0.2).exp())).rename('DistScore')\n\n            aero = img.select('AERO')\n            aot = ee.Image(ee.Algorithms.If(\n                isL57, aero.multiply(0.001),\n                aero.bitwiseAnd(192).rightShift(6).expression(\n                    \"b(0)==0?0.05:b(0)==1?0.1:b(0)==2?0.2:b(0)==3?0.3:0\",\n                    {'b(0)': aero.bitwiseAnd(192).rightShift(6)}))\n            ).rename('AOT_Debug')\n\n            Omin, Omax = ee.Number(0.2), ee.Number(0.3)\n            Omid = Omin.add(Omax).divide(2)\n            opacity_score = ee.Image(1).subtract(\n                ee.Image(1).divide(ee.Image(1).add(aot.min(Omax).subtract(Omid).multiply(-4).exp()))\n            ).where(aot.lte(Omin), 1).where(aot.gte(Omax), 0).rename('OpacityScore')\n\n            doy = ee.Number(date.getRelative('day', 'year'))\n            mu, sigma = ee.Number(213), ee.Number(38)\n            norm = sigma.multiply((2 * 3.14159)**0.5)\n            max_score = ee.Number(1).divide(norm)\n            raw = doy.subtract(mu).pow(2).divide(sigma.pow(2)).multiply(-0.5).exp()\n            doy_score = raw.divide(norm).divide(max_score)\n            doy_score_img = ee.Image.constant(doy_score).toFloat().rename('DOYScore')\n\n            total_score = sensor_score.multiply(slc_penalty).add(dist_score).add(opacity_score).add(doy_score_img).rename('TotalScore')\n\n            return img.addBands([ndvi, evi, nbr, dist_score, opacity_score, doy_score_img, total_score])\n\n        return collection.map(score_image)\n\n    def get_annual_best(year):\n        scored = load_and_score_landsat(year)\n        best = scored.qualityMosaic('TotalScore').clip(tile_geom)\n        return best.select(['NDVI', 'EVI', 'NBR']) \\\n                   .set('year', year) \\\n                   .set('tile_id', tile_id) \\\n                   .set('system:time_start', ee.Date.fromYMD(year, 7, 1).millis())\n\n    annual_best = ee.ImageCollection(year_list.map(get_annual_best))\n\n    def smooth_collection(coll):\n        return ee.ImageCollection(year_list.map(\n            lambda y: coll.filter(ee.Filter.eq('year', y)).first()\n                      .set('year', y)\n                      .set('system:time_start', ee.Date.fromYMD(y, 7, 1).millis())\n        ))\n\n    return smooth_collection(annual_best)\n\n#  Build Full Smoothed Collection from All Tiles\ntile_indices = ee.List.sequence(0, n_tiles.subtract(1))\n\nimage_lists = tile_indices.map(lambda i: process_tile(i).toList(process_tile(i).size()))\nflattened_images = image_lists.flatten()\n\n#Before collecting the into the collection below other post-processing were performed,\n#including despiking, multi-factor wieghted infiling, etc {full details can be provided upon reasonable request}\n\nfull_smoothed_collection = ee.ImageCollection.fromImages(flattened_images)\n\nprint(\" Full Smoothed ImageCollection Ready\")\n","metadata":{"id":"XALOl_14ce8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Se4C8EyaddI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  9.1 Compute Trend & Base + Z-Significant Classification Per Tile + Return SenSlope & ZScore\n\ndef compute_veg_classification(tile_feature):\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    # Filter smoothed collection for this tile\n    tile_col = full_smoothed_collection.filter(ee.Filter.eq('tile_id', tile_id))\n    years = ee.List(tile_col.aggregate_array('year')).sort()\n\n    # Build ImageCollection for trend analysis\n    def build_trend_image(y):\n        y = ee.Number(y)\n        img = tile_col.filter(ee.Filter.eq('year', y)).first()\n        rel_year = y.subtract(start_year)\n        return img.select('NDVI').addBands(\n            ee.Image.constant(rel_year).rename('relativeYear').toFloat()\n        )\n    imgs_for_trend = ee.ImageCollection(years.map(build_trend_image))\n\n    # Sen's Slope and Kendall's Tau\n    sen = imgs_for_trend.select(['relativeYear', 'NDVI']) \\\n        .reduce(ee.Reducer.sensSlope()).select('slope').rename('SenSlope')\n\n    tau = imgs_for_trend.select(['relativeYear', 'NDVI']) \\\n        .reduce(ee.Reducer.kendallsCorrelation()).select('NDVI_tau').rename('KendallTau')\n\n    N = ee.Number(end_year).subtract(start_year).add(1)\n    z_score = tau.multiply(3).multiply(N.multiply(N.subtract(1)).sqrt()) \\\n        .divide(ee.Number(2).multiply(N.multiply(2).add(5).sqrt())) \\\n        .rename('KendallZ')\n\n    # Threshold-based classifications\n    greening = sen.gt(0.0015).And(z_score.abs().gt(0.52))\n    browning = sen.lt(-0.0015).And(z_score.abs().gt(0.52))\n    stable = sen.gte(-0.0015).And(sen.lte(0.0015)).And(z_score.abs().gt(0.52))\n\n    first_ndvi = tile_col.sort('year').first().select('NDVI')\n    last_ndvi = tile_col.sort('year', False).first().select('NDVI')\n    ndvi_drop = last_ndvi.subtract(first_ndvi)\n    persistent_decline = ndvi_drop.lt(-0.0015)\n\n    greening_class = greening.And(slope_green_image.clip(tile_geom).gte(0.0015))\n    browning_class = browning.And(\n        slope_brown_image.clip(tile_geom).lte(-0.0015)\n    ).And(persistent_decline)\n    stable_class = stable.And(\n        slope_green_image.clip(tile_geom).lte(0.0015)\n    ).And(\n        slope_brown_image.clip(tile_geom).gte(-0.0015)\n    )\n\n    veg_class = ee.Image(0) \\\n        .where(greening_class, 1) \\\n        .where(browning_class, 2) \\\n        .where(stable_class, 3) \\\n        .rename('VegChangeClass') \\\n        .set('tile_id', tile_id)\n\n    #  Z-Score based classification\n    significant = z_score.abs().gte(1.62)\n\n    ndvi_trend_zsen = ee.Image(0) \\\n        .where(significant.And(sen.lt(-0.003)), 1) \\\n        .where(significant.And(sen.gte(-0.003).And(sen.lt(0))), 2) \\\n        .where(significant.And(sen.eq(0)), 3) \\\n        .where(significant.And(sen.gt(0).And(sen.lte(0.003))), 4) \\\n        .where(significant.And(sen.gt(0.003)), 5) \\\n        .rename('NDVI_Trend_ZSen')\n\n    return veg_class \\\n        .addBands(ndvi_trend_zsen) \\\n        .addBands(sen) \\\n        .addBands(z_score) \\\n        .set('tile_id', tile_id) \\\n        .set('system:time_start', ee.Date.fromYMD(end_year, 7, 1).millis()) \\\n        .clip(tile_geom)\n","metadata":{"id":"eCxBWgp9d15b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"pvw9zocBd11k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Export NDVI KendallZ per tile (Drive) -------------------------------\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()\n\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]  # ~30 m\n\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    try:\n        tile_id_val = tile.get('tile_id').getInfo()\n    except Exception:\n        tile_id_val = i + 1\n\n    # Get the raw Kendall Z band from your function\n    ndvi_kz_tile = compute_veg_classification(tile).select('KendallZ')\n\n    # Export as real NoData (no unmask, no updateMask)\n    image_to_export = ndvi_kz_tile.toFloat().clip(tile_geom)\n\n    desc  = f'NDVI_KendallZ_{start_year}_{end_year}_{tile_id_val}'\n    fname = f'NDVI_KendallZ_{start_year}_{end_year}_{tile_id_val}'\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=desc,\n        folder='NDVI_KendallZ_30m',\n        fileNamePrefix=fname,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        fileFormat='GeoTIFF',\n        maxPixels=1e13\n    )\n    task.start()\n    print(f\" Export task submitted for tile {tile_id_val}: {desc}\")\n\nprint(\"All export tasks submitted.\")\n","metadata":{"id":"GUeLaK8Jd1xN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ee\nimport geemap\n\n#  Initialize Earth Engine\nee.Initialize()\n\n#  Load tiles and NDVI mosaic for 1986\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()\n\n# NDVI 1986 mosaic (previously generated)\nndvi_1986 = full_smoothed_collection \\\n    .filter(ee.Filter.eq('year', 1986)) \\\n    .select('NDVI') \\\n    .mosaic() \\\n    .clip(tiles_fc.geometry())\n\n# ðŸ›  Export settings\nno_data_value = 9999\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]  # â‰ˆ30m\n\n#  Loop through each tile and export\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1\n\n    description = f'NDVI_1986_{tile_id}'\n    file_name_prefix = f'NDVI_1986_{tile_id}'\n\n    image_to_export = ndvi_1986.unmask(no_data_value).toFloat().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='Starting_NDVI_30_1',\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\"All export tasks submitted. Go to the Earth Engine Code Editor 'Tasks' tab to run them manually.\")\n","metadata":{"id":"Q1E7GJFtddEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"by_tile_veg_class = ee.ImageCollection(tile_list.map(lambda f: compute_veg_classification(f)))","metadata":{"id":"I2W6t7rejUIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_veg_class = by_tile_veg_class.select('VegChangeClass').mosaic()\nndvi_trend_zsen = by_tile_veg_class.select('NDVI_Trend_ZSen').mosaic()\nstatic_sen_slope = by_tile_veg_class.select('SenSlope').mosaic()\nstatic_z_score = by_tile_veg_class.select('KendallZ').mosaic()\n","metadata":{"id":"Bzj_51Jww7sT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_tile_with_fire_correction(tile_index):\n    tile_index = ee.Number(tile_index)\n    tile_feature = tile_list.get(tile_index)  #  Correct: No need to wrap with ee.Feature\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    fire_occurrence = fire_raster.gte(1986).selfMask().clip(tile_geom)\n    fire_year = fire_raster.clip(tile_geom).rename('fireYear')\n\n    def build_img(y):\n        y = ee.Number(y)\n        img = full_smoothed_collection \\\n            .filter(ee.Filter.eq('year', y)) \\\n            .filter(ee.Filter.eq('tile_id', tile_id)).first()\n        rel_year = y.subtract(start_year)\n        return img.select('NDVI') \\\n            .addBands(ee.Image.constant(y).rename('absoluteYear').toFloat()) \\\n            .addBands(ee.Image.constant(rel_year).rename('relativeYear').toFloat()) \\\n            .clip(tile_geom)\n\n    imgs = ee.ImageCollection(year_list.map(build_img))\n\n    pre_fire = imgs.map(lambda img: img.updateMask(img.select('absoluteYear').lt(fire_year))) \\\n        .filter(ee.Filter.listContains('system:band_names', 'NDVI'))\n\n    post_fire = imgs.map(lambda img: img.updateMask(img.select('absoluteYear').gt(fire_year))) \\\n        .filter(ee.Filter.listContains('system:band_names', 'NDVI'))\n\n    pre_count = pre_fire.size()\n    post_count = post_fire.size()\n\n    # Pre-fire slope & Z\n    pre_sen = pre_fire.select(['relativeYear', 'NDVI']).reduce(ee.Reducer.sensSlope()).select('slope')\n    pre_tau = pre_fire.select(['relativeYear', 'NDVI']).reduce(ee.Reducer.kendallsCorrelation()).select('NDVI_tau')\n    pre_z = pre_tau.multiply(3).multiply(pre_count.multiply(pre_count.subtract(1)).sqrt()) \\\n        .divide(ee.Number(2).multiply(pre_count.multiply(2).add(5)).sqrt()).rename('PreFire_Zscore')\n\n    # Post-fire slope & Z\n    post_sen = post_fire.select(['relativeYear', 'NDVI']).reduce(ee.Reducer.sensSlope()).select('slope')\n    post_tau = post_fire.select(['relativeYear', 'NDVI']).reduce(ee.Reducer.kendallsCorrelation()).select('NDVI_tau')\n    post_z = post_tau.multiply(3).multiply(post_count.multiply(post_count.subtract(1)).sqrt()) \\\n        .divide(ee.Number(2).multiply(post_count.multiply(2).add(5)).sqrt()).rename('PostFire_Zscore')\n\n    pre_sig = pre_z.abs().gt(0.52)\n    post_sig = post_z.abs().gt(0.52)\n    both_sig = pre_sig.And(post_sig)\n\n    weight_total = pre_count.add(post_count)\n    final_slope = pre_sen.multiply(pre_count).add(post_sen.multiply(post_count)) \\\n        .divide(weight_total).rename('FireAdjusted_Slope').updateMask(both_sig)\n\n    green = final_slope.gte(0.0015)\n    brown = final_slope.lte(-0.0015)\n    stable = final_slope.gt(-0.005).And(final_slope.lte(0.0015))\n\n    fire_correction = ee.Image(0) \\\n        .where(green, 1) \\\n        .where(brown, 2) \\\n        .where(stable, 3) \\\n        .updateMask(both_sig.And(fire_occurrence))\n\n    corrected_class = base_veg_class.where(fire_correction.neq(0), fire_correction).rename('Final_VegChange')\n\n    trend_class = ee.Image(0) \\\n        .where(final_slope.lt(-0.003), 1) \\\n        .where(final_slope.gte(-0.003).And(final_slope.lt(0)), 2) \\\n        .where(final_slope.eq(0), 3) \\\n        .where(final_slope.gt(0).And(final_slope.lte(0.003)), 4) \\\n        .where(final_slope.gt(0.003), 5) \\\n        .rename('FireAdjusted_TrendClass')\n\n    return final_slope.rename('FireAdjusted_Slope') \\\n        .addBands(pre_sen.rename('PreFire_Slope')) \\\n        .addBands(post_sen.rename('PostFire_Slope')) \\\n        .addBands(pre_z) \\\n        .addBands(post_z) \\\n        .addBands(corrected_class) \\\n        .addBands(trend_class) \\\n        .set('tile_id', tile_id) \\\n        .set('system:time_start', ee.Date.fromYMD(end_year, 7, 1).millis()) \\\n        .clip(tile_geom)\n","metadata":{"id":"YUYAZlo6w7w_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"rXNQy5joxPS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = ee.List.sequence(0, tile_list.size().subtract(1))\nfire_corrected_tiles = ee.ImageCollection(\n    indices.map(lambda i: process_tile_with_fire_correction(i))\n)\n","metadata":{"id":"0H6sqI2cxPXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"FbONrSxoxTjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fire_adjusted_slope = fire_corrected_tiles.select('FireAdjusted_Slope').mosaic()\npre_slope = fire_corrected_tiles.select('PreFire_Slope').mosaic()\npost_slope = fire_corrected_tiles.select('PostFire_Slope').mosaic()\npre_z = fire_corrected_tiles.select('PreFire_Zscore').mosaic()\npost_z = fire_corrected_tiles.select('PostFire_Zscore').mosaic()\nfire_corrected_veg = fire_corrected_tiles.select('Final_VegChange').mosaic()\nreclassified_trend = fire_corrected_tiles.select('FireAdjusted_TrendClass').mosaic()\n","metadata":{"id":"LKCQB45fxTtq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"GHV5rjTRxT4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fire_occurrence = fire_raster.gte(1986).selfMask()\nfinal_static_slope = static_sen_slope.where(\n    fire_occurrence, fire_adjusted_slope\n).rename('Final_Static_SenSlope')\n","metadata":{"id":"ZvAlnuKjxYNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boolean mask: True where |Z| = 0.84 (non-significant)\nnon_significant_bool = static_z_score.abs().lt(0.84).rename('ZScore_NonSignificant')","metadata":{"id":"kHWtiWYhxYIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Boolean mask where absolute Z-score < 0.84\nzscore_nonsig_mask = static_z_score.abs().lt(0.84).selfMask()","metadata":{"id":"8Ix3F9IjxYDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_lineartrend_class = ee.Image(0) \\\n    .where(final_static_slope.lte(-0.003), 1) \\\n    .where(final_static_slope.gt(-0.003).And(final_static_slope.lte(-0.0015)), 2) \\\n    .where(final_static_slope.gt(-0.0015).And(final_static_slope.lt(0.0015)), 3) \\\n    .where(final_static_slope.gte(0.0015).And(final_static_slope.lt(0.003)), 4) \\\n    .where(final_static_slope.gte(0.003), 5)\n","metadata":{"id":"2VrPD3vGxX2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#building of masks to filter the trend grids\n#  1. Load region of interest\nregion = ee.FeatureCollection(\"projects/danielsiglab/assets/studyarea50\")\nregion_geom = region.geometry()\n\n#  2. Define years and remapping\nyears = list(range(1984, 2023))\noriginal_classes = [0, 20, 31, 32, 33, 40, 50, 80, 81, 100, 210, 220, 230]\nremap_classes = list(range(13))\nmask_classes = [1, 2]\n\n# 3. Get reclassified image per year\ndef get_reclass_lc(year):\n    img = ee.ImageCollection(\"projects/sat-io/open-datasets/CA_FOREST_LC_VLCE2\") \\\n        .filter(ee.Filter.calendarRange(year, year, 'year')) \\\n        .first() \\\n        .select('b1') \\\n        .remap(original_classes, remap_classes) \\\n        .rename('LC') \\\n        .set('year', year)\n    return img\n\n#  Reclassify and build collection\nlc_collection = ee.ImageCollection([get_reclass_lc(y) for y in years])\n\n# Create persistent water/snow mask\nmask_collection = lc_collection.map(lambda img: img.remap(mask_classes, [1, 1], 0).rename('Mask'))\nsum_mask = mask_collection.reduce(ee.Reducer.sum())\npersistent_mask = sum_mask.eq(len(years)).rename('Persistent_Mask')\nfinal_water_snow_mask = persistent_mask.updateMask(persistent_mask.neq(0)).unmask(0).rename('Persistent_Mask')\n\n# Load built-up surface for 2020 and clip\nbuilt_2020 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/2020').select('built_surface')\nbuilt_clipped = built_2020.clip(region_geom)\nbuilt_mask = built_clipped.gt(0).rename('built_mask')\n\n#  Combine masks\ncombined_mask = final_water_snow_mask.max(built_mask).rename('Combined_Mask')\n\n# Export to asset\n#task = ee.batch.Export.image.toAsset(\n    #image=combined_mask.clip(region_geom),\n    #description='Export_Combined_Mask_BuiltUp_WaterSnow',\n    #assetId='projects/danielsiglab/assets/combined_builtup_water_snow_mask',\n    #region=region_geom,\n    #scale=30,\n    #maxPixels=1e13\n#)\n#task.start()\n#print(\"Export task started. Monitor it from the Earth Engine Tasks tab.\")\n","metadata":{"id":"HPXy5F6xjT_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DtMKAdWInTpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reclassify lineartrend_class where combined_mask == 1 to value 6\nfinal_lineartrend_class = initial_lineartrend_class.where(combined_mask.eq(1), 6).rename('LinearTrend_Class_Corrected')\n","metadata":{"id":"Oe6gFn4-nUCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create binary ecozone raster (1 = inside, 0 = outside)\necozone_mask = ee.Image.constant(1).clip(full_ecozone_fc).unmask(0).rename('Ecozone_Mask')","metadata":{"id":"SLH8nL8BnUg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8TNpfhY2ouAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Reclassify linear trend class using the ecozone mask\nfinal_class2 = final_lineartrend_class \\\n    .where(ecozone_mask.eq(0), 7) \\\n    .rename('LinearTrend_Class_Corrected_Final')","metadata":{"id":"m-fm6i8fnXru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"5m2pGdig5tR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the unioned geometry of all tiles\ntiles_extent = tiles_fc.geometry()\n\n# Step 2: Clip the final class image to the tile extent\nfinal_class2_clipped = final_class2.clip(tiles_extent)\n","metadata":{"id":"7r5dIKWIo1PL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KpLQR5HWo1Ss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To export the linear class properly thresholded","metadata":{"id":"EGuVZp7T3uBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Load final classified image (assumed already computed)\n# Load tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()  # Convert to client-side integer\n\n# Export settings\nno_data_value = 9999\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n#  Loop through tiles and create export tasks\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1  # Sequential numbering from 1\n\n    description = f'linearclass_{tile_id}'\n    file_name_prefix = f'linearclass_{tile_id}'\n\n    image_to_export = final_class2_clipped.unmask(no_data_value).toInt16().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='LINEAR_TREND',  # Folder name updated\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\"All export tasks submitted. Go to the Earth Engine Code Editor 'Tasks' tab to run them manually.\")\n","metadata":{"id":"_c3mVVnT44-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To export the linear slope grid","metadata":{"id":"v5WTYe3P45CP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Load tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()\n\n#  Export settings\nno_data_value = 0  # Suitable for float rasters\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n# Loop through tiles and export slope\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1\n\n    description = f'linearslope_{tile_id}'\n    file_name_prefix = f'linearslope_{tile_id}'\n\n    image_to_export = final_static_slope_clipped.unmask(no_data_value).toFloat().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='linear_slope',  # Folder name for slope\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\"All slope export tasks submitted. Check the Earth Engine 'Tasks' tab to monitor.\")\n","metadata":{"id":"72TAhISK45GW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"cZECzYYp45Jd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To fit the exponential trend model","metadata":{"id":"SLDPtt2J60tR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ee\n\n#  10.6 Tile-Based Static Exponential Growth Fitting (Per Pixel)\ndef per_tile_exp_fit(tile_index):\n    tile_index = ee.Number(tile_index)\n    tile_feature = ee.Feature(tile_list.get(tile_index))\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    # Filter the smoothed NDVI collection for this tile\n    tile_col = full_smoothed_collection.filter(ee.Filter.eq('tile_id', tile_id))\n    years = ee.List(tile_col.aggregate_array('year')).sort()\n\n    # Build image collection with log(NDVI) and relative year\n    def build_log_img(y):\n        y = ee.Number(y)\n        img = tile_col.filter(ee.Filter.eq('year', y)).first()\n        rel_year = ee.Image.constant(y.subtract(start_year)).rename('relativeYear').toFloat()\n        log_ndvi = img.select('NDVI').log().rename('log_NDVI')\n        return log_ndvi.addBands(rel_year).clip(tile_geom)\n\n    imgs_for_fit = ee.ImageCollection(years.map(build_log_img))\n\n    # Linear fit in log-space: log(NDVI) = b * t + log(a)\n    fit = imgs_for_fit.select(['relativeYear', 'log_NDVI']).reduce(ee.Reducer.linearFit())\n    slope = fit.select('scale').rename('Exp_Slope')              # b\n    intercept = fit.select('offset').rename('Exp_Intercept')     # log(a)\n\n    # Back-transform slope to approximate per-year growth rate: exp(b) - 1\n    transformed_slope = slope.exp().subtract(1).rename('Exp_Slope_Transformed')\n\n    # Compute MSE (Mean Squared Error) in original NDVI space\n    def compute_sq_error(img):\n        obs = img.select('log_NDVI').exp()\n        pred = intercept.add(slope.multiply(img.select('relativeYear'))).exp()\n        return obs.subtract(pred).pow(2).rename('SqError')\n\n    mse = ee.ImageCollection(imgs_for_fit.map(compute_sq_error)).mean().rename('Exp_RMSE')\n\n    # Compute TSS (Total Sum of Squares)\n    mean_ndvi = tile_col.select('NDVI').mean()\n    def tss_img(img):\n        obs = img.select('NDVI')\n        return obs.subtract(mean_ndvi).pow(2).rename('TSS')\n\n    tss_mean = ee.ImageCollection(tile_col.map(tss_img)).mean().rename('TSS_Mean')\n\n    # Compute RÂ² = 1 - (MSE / TSS)\n    r_squared = ee.Image(1).subtract(mse.divide(tss_mean)).rename('Exp_R2')\n\n    # Return all bands for the tile\n    return transformed_slope.addBands(intercept).addBands(r_squared).addBands(mse) \\\n        .clip(tile_geom).set('tile_id', tile_id)\n\n# Apply exponential fitting to all tiles\nexp_fit_tiles = ee.ImageCollection.fromImages(\n    ee.List.sequence(0, n_tiles.subtract(1)).map(per_tile_exp_fit)\n)\n\n# Merge into one per-pixel layer\nper_pixel_exp_stack = exp_fit_tiles.mosaic()\n\n","metadata":{"id":"yLlhyJT_60qQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select RÂ² from exponential and logistic fits\nexp_r2 = per_pixel_exp_stack.select('Exp_R2')\n# Mask pixels with Exp_R2 â‰¥ 0.5\nexp_significant = exp_r2.gte(0.6).selfMask()\n\n# Add layers to the map\nMap.addLayer(exp_r2, {\n    'min': 0, 'max': 1,\n    'palette': ['white', 'limegreen']\n}, 'Exponential RÂ²')\n\nMap.addLayer(exp_significant, {\n    'min': 0, 'max': 1,\n    'palette': ['red']\n}, 'Exp RÂ² â‰¥ 0.5')\n\nMap","metadata":{"id":"4M6Dp1Dy7ujC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 10.8 Per-Pixel Logistic NDVI Fitting with pImage as c (Relative Year Adjustment)\n\ndef logistic_fit_tile(tile_index):\n    tile_feature = ee.Feature(tile_list.get(tile_index))\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.get('tile_id')\n\n    tile_col = full_smoothed_collection.filter(ee.Filter.eq('tile_id', tile_id))\n    years = ee.List(tile_col.aggregate_array('year')).sort()\n\n    ndvi_min = tile_col.select('NDVI').reduce(ee.Reducer.min())\n    ndvi_max = tile_col.select('NDVI').reduce(ee.Reducer.max())\n\n    p_image_tile = pimage.clip(tile_geom)\n    c_image = p_image_tile.subtract(start_year).rename('c').toFloat()\n\n    norm_ndvi = tile_col.map(lambda img:\n        img.select('NDVI').subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).rename('Norm_NDVI')\n        .addBands(ee.Image.constant(ee.Number(img.get('year')).subtract(start_year)).rename('relYear').toFloat())\n        .clip(tile_geom)\n    )\n\n    logit_collection = norm_ndvi.map(lambda img:\n        img.select('Norm_NDVI').divide(ee.Image(1).subtract(img.select('Norm_NDVI'))).log().rename('Logit_NDVI')\n        .addBands(img.select('relYear'))\n    )\n\n    fit = logit_collection.select(['relYear', 'Logit_NDVI']).reduce(ee.Reducer.linearFit())\n    b = fit.select('scale').rename('b')\n    d = fit.select('offset').rename('d')\n\n    log_slope_trans = b.exp().subtract(1).rename('Log_Slope_Transformed')\n    a = ndvi_max.subtract(ndvi_min).rename('a')\n\n    mse = ee.ImageCollection(norm_ndvi.map(lambda img: (\n        img.select('Norm_NDVI')\n        .subtract(a.divide(ee.Image(1).add(b.multiply(img.select('relYear').subtract(c_image)).multiply(-1).exp())).add(d))\n        .pow(2).rename('SqError')\n    ))).mean().rename('Logistic_RMSE')\n\n    mean_obs = norm_ndvi.select('Norm_NDVI').mean()\n    tss = norm_ndvi.map(lambda img: img.select('Norm_NDVI').subtract(mean_obs).pow(2).rename('TSS'))\n    tss_mean = ee.ImageCollection(tss).mean().rename('TSS_Mean')\n\n    r2 = ee.Image(1).subtract(mse.divide(tss_mean)).rename('Logistic_R2')\n\n    return log_slope_trans.addBands(c_image).addBands(r2).addBands(mse).addBands(a).addBands(d).clip(tile_geom).set('tile_id', tile_id)\n\n# Run across all tiles\nlogistic_fit_tiles = ee.ImageCollection.fromImages(\n    ee.List.sequence(0, n_tiles.subtract(1)).map(lambda i: logistic_fit_tile(i))\n)\n\n# Mosaic results\nper_pixel_logistic_stack = logistic_fit_tiles.mosaic()\n\nMap.addLayer(per_pixel_logistic_stack.select('Log_Slope_Transformed'),\n             {'min': -0.01, 'max': 0.01, 'palette': ['blue', 'white', 'orange']},\n             'Logistic Slope Transformed')\nprint('Logistic Fitting with Slope Transformation Completed.')\n\n","metadata":{"id":"o_F_dG3F85Wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"y9YaysVh85hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_significant = per_pixel_exp_stack.select('Exp_R2').gte(0.6)\n","metadata":{"id":"jh08WmiM70Z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_slope = per_pixel_exp_stack.select('Exp_Slope_Transformed')\n\nexp_class = ee.Image(0) \\\n    .where(exp_slope.lte(-0.002), 1) \\\n    .where(exp_slope.gt(-0.002).And(exp_slope.lte(-0.00132)), 2) \\\n    .where(exp_slope.gt(-0.00132).And(exp_slope.lt(0.00132)), 3) \\\n    .where(exp_slope.gte(0.00132).And(exp_slope.lt(0.002)), 4) \\\n    .where(exp_slope.gte(0.002), 5)\n","metadata":{"id":"rVMpLP2A8BSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"c7mL0kk18EpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_class_final = exp_class.where(exp_significant.Not(), 0).rename('Exp_Trend_Class')\n","metadata":{"id":"hXcpLnrF8Etn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Step 1: Compute the boolean mask: where linear trend is 0 and exponential class is > 0\nnew_exp_only = lineartrend_class.eq(0).And(exp_class_final.gt(0))\n\n# Step 2: Clip it to full_ecozone_fc and convert to 0/1 values\nnew_exp_only_clipped = new_exp_only.clip(full_ecozone_fc).unmask(0).int8().rename('Exp_Only_Clipped')\n","metadata":{"id":"KigJmcoS8Exp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"j7pkuUCM8E44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_stack = per_pixel_exp_stack\nexp_r2 = exp_stack.select('Exp_R2')\nexp_slope = exp_stack.select('Exp_Slope_Transformed')\n","metadata":{"id":"nqp4qm8o8E8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_significant = exp_r2.gte(0.6)\n","metadata":{"id":"YVPOHjNt8E_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_no_trend = lineartrend_class.eq(0)\n","metadata":{"id":"JTxxfnFX8Bc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"condition_1 = linear_no_trend.And(exp_significant)\n","metadata":{"id":"ywrkrBC28BgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_ndvi = full_smoothed_collection.filter(ee.Filter.eq('year', 1986)).first().select('NDVI')\n","metadata":{"id":"V_lPajpo8BkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"years = ee.Number(2023).subtract(1986)\n\n# Linear NDVI change over 37 years\nlinear_change = final_static_slope.multiply(years)\n\n# Exponential NDVI change: Î”NDVI = NDVIâ‚€ * [(1 + r)^t - 1]\nexp_multiplier = exp_slope.add(1).pow(years)\nexp_change = initial_ndvi.multiply(exp_multiplier.subtract(1))\n\n# Condition 2: where exponential change is significantly larger than linear\ncondition_2 = exp_change.subtract(linear_change).gt(0.02)\n","metadata":{"id":"j33NS-dE8nJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_exponential = condition_1.Or(condition_2)\n","metadata":{"id":"vLoo5Z8C8nUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_slope = per_pixel_logistic_stack.select('Log_Slope_Transformed')\n\nlog_class = ee.Image(0) \\\n    .where(log_slope.lte(-0.002), 1) \\\n    .where(log_slope.gt(-0.002).And(log_slope.lte(-0.00132)), 2) \\\n    .where(log_slope.gt(-0.00132).And(log_slope.lt(0.00132)), 3) \\\n    .where(log_slope.gte(0.00132).And(log_slope.lt(0.002)), 4) \\\n    .where(log_slope.gte(0.002), 5)\n","metadata":{"id":"JnjZDKwz8nYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"v_wklCZL8nbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the RÂ² band from the logistic fit stack\nlogistic_r2 = per_pixel_logistic_stack.select('Logistic_R2')\n\n# Create a significance mask: True where RÂ² â‰¥ 0.5\nlogistic_significant = logistic_r2.gte(0.6).rename('Logistic_Significant')\n","metadata":{"id":"9qTu62g18nep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Bfj8e7Cp8nhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_class_final = log_class.where(logistic_significant.Not(), 0).rename('Log_Trend_Class')\n","metadata":{"id":"neUstcFN8nkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geemap\n\n# Clip the exponential slope classification to the ecozone extent\nlog_class_clipped = log_class_final.clip(full_ecozone_fc)\n\n#  Define the visualization parameters using the same palette as `lineartrend_vis`\nexptrend_vis = {\n    'min': 0,\n    'max': 5,\n    'palette': [\n        '#A9A9A9',  # Gray - No significant trend\n        '#8B4513',  # SaddleBrown - Strong negative\n        '#DAA520',  # GoldenRod - Moderate negative\n        '#4682B4',  # SteelBlue - Near stable\n        '#90EE90',  # LightGreen - Moderate positive\n        '#006400'   # DarkGreen - Strong positive\n    ]\n}\n\n#  Create a new map instance to avoid overlap\nlogSlopeMap = geemap.Map()\n\n# Add the clipped classification layer\nlogSlopeMap.addLayer(log_class_clipped, exptrend_vis, ' log Slope Class (Clipped & Colored)')\n\n#Display the new standalone map\nlogSlopeMap\n\n","metadata":{"id":"QnzKdUdF8nop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8Ug5m8_69LjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Compute NDVI baseline and final means\nndvi_early = full_smoothed_collection \\\n    .filter(ee.Filter.inList('year', [1986, 1987, 1988])) \\\n    .select('NDVI') \\\n    .mean().rename('NDVI_Early')\n\nndvi_late = full_smoothed_collection \\\n    .filter(ee.Filter.inList('year', [2021, 2022, 2023])) \\\n    .select('NDVI') \\\n    .mean().rename('NDVI_Late')\n\nobserved_change = ndvi_late.subtract(ndvi_early)\n\n# Compute modeled NDVI changes\nyears_diff = ee.Number(2023 - 1986)\n\n# Linear model NDVI estimate\nlinear_slope = final_static_slope\nlinear_predicted_change = linear_slope.multiply(years_diff)\nndvi_linear_predicted = ndvi_early.add(linear_predicted_change)\n\n# Exponential model NDVI estimate\nexp_r2 = per_pixel_exp_stack.select('Exp_R2')\nexp_slope = per_pixel_exp_stack.select('Exp_Slope_Transformed')\nexp_significant = exp_r2.gte(0.6)\n\nexp_multiplier = exp_slope.add(1).pow(years_diff)\nndvi_exp_predicted = ndvi_early.multiply(exp_multiplier)\n\n# Step 3: Compare models to observed NDVI\ndiff_linear = ndvi_linear_predicted.subtract(ndvi_late).abs()\ndiff_exp = ndvi_exp_predicted.subtract(ndvi_late).abs()\n\n# Select pixels where exponential is a better fit\nexp_better_fit = diff_exp.lt(diff_linear).And(exp_significant)\n\n# Also include where linear had no trend (class 0)\nlinear_no_trend = lineartrend_class.eq(0)\ncombined_condition = linear_no_trend.Or(exp_better_fit)\n\n# Update final class and/or slope map\nupdated_trend_class = lineartrend_class.where(combined_condition, exp_class_final)\nupdated_slope = final_static_slope.where(combined_condition, exp_slope)\n\n# Optional: visualize or export\n#Map.addLayer(combined_condition.selfMask(), {'palette': ['orange']}, 'Pixels better explained by Exp')\nMap.addLayer(combined_condition, {'min': 0, 'max': 8}, 'Updated Trend Class')\nMap","metadata":{"id":"jlYhvUlw9LnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Qae3cxMK9SzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define visualization style for boolean mask\ncombined_vis = {\n    'min': 0,\n    'max': 1,\n    'palette': ['gray', 'orange']  # 0 = keep linear, 1 = use exponential\n}\n\n#  Create a new map to isolate this visualization\ncombinedMap = geemap.Map()\n\n#  Add the combined condition grid\ncombinedMap.addLayer(combined_condition, combined_vis, 'Better Explained by Exp or No Trend')\n\n#  Display\ncombinedMap\n","metadata":{"id":"69vaMfnZ9TBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"L8fxS_PJ9TF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Create a new map instance to isolate exponential-better-only pixels\nexpBetterOnlyMap = geemap.Map(center=[60, -96], zoom=3, height=\"1000px\")\n\n#  Clip to ecozone boundaries to match styling\nexp_better_masked_full = exp_better_fit.clip(full_ecozone_fc)\n\n#  Define two-class visualization style: 0 = linear better, 1 = exp better\nexp_better_vis = {\n    'min': 0,\n    'max': 1,\n    'palette': ['#D3D3D3', 'red']  # light gray for linear, red for exponential\n}\n\n# Add to map\nexpBetterOnlyMap.addLayer(exp_better_masked_full, exp_better_vis, ' Exp vs Linear Fit Comparison')\n\n# Add boundaries\nexpBetterOnlyMap.addLayer(\n    full_ecozone_fc.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Ecozone Boundary'\n)\n\ncanada_country = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\").filter(ee.Filter.eq(\"country_na\", \"Canada\"))\ncanada_provinces = ee.FeatureCollection(\"FAO/GAUL/2015/level1\").filter(ee.Filter.eq(\"ADM0_NAME\", \"Canada\"))\n\nexpBetterOnlyMap.addLayer(\n    canada_country.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Canada Border'\n)\n\nexpBetterOnlyMap.addLayer(\n    canada_provinces.style(**{'color': 'gray', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Province Boundaries'\n)\n\n# Add a scale bar\nfrom ipyleaflet import ScaleControl\nexpBetterOnlyMap.add_control(ScaleControl(position='bottomleft'))\n\n#  Add updated legend for Exp vs Linear better fit\nfrom ipywidgets import HTML\nlegend_html = HTML(\n    value=\"\"\"\n    <div style='background:white; font-size:16px; padding:10px; border:none; margin-left:40px;'>\n        <b>Pixel Best Fit </b><br>\n        <div><span style='background:#D3D3D3; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Linear</div>\n        <div><span style='background:red; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Exponential</div>\n    </div>\n    \"\"\"\n)\n\nfrom ipyleaflet import WidgetControl\nexpBetterOnlyMap.add_control(WidgetControl(widget=legend_html, position='bottomleft'))\n\n# Display the map\nexpBetterOnlyMap\n","metadata":{"id":"Jw-_Qc8V9LrL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"M5gsMSzk9Lxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Convert exp_better_fit (boolean) to class values\n# 1 = exponential better fit, 0 = linear better\nexp_better_class = exp_better_fit.where(exp_better_fit, 1).unmask(0).uint8()\n\n# Step 7: Identify pixels where exponential recovered NDVI in linear class 0\n# => exponential RÂ² â‰¥ 0.5 and linear model originally showed no trend\nexp_class2_mask = linear_no_trend.And(exp_significant)\n\n# Step 8: Assign class 2 where exponential recovered linear class 0\n# Only replace pixels originally class 0 (i.e., where exp_better_class == 0)\nexp_3class_grid = exp_better_class.where(\n    exp_better_class.eq(0).And(exp_class2_mask),\n    2\n).uint8()\n","metadata":{"id":"UKa2eXqA9L0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"3VKyE0sB9vR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Bands in exp_better_fit:\", exp_better_fit.bandNames().getInfo())\nprint(\"Bands in exp_significant:\", exp_significant.bandNames().getInfo())\n","metadata":{"id":"j2desabg9vbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"sE2dnMZ29vec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define visualization style for the 3-class grid\n# 0 = Linear better, 1 = Exp better, 2 = Exp recovered from linear 0\nexp_3class_vis = {\n    'min': 0,\n    'max': 2,\n    'palette': ['#D3D3D3', '#FF0000', '#32CD32']  # gray, red, lime green\n}\n\n# Create a new map instance\nexp3classMap = geemap.Map(center=[60, -96], zoom=3, height=\"1000px\")\n\n# Clip to ecozone boundaries for styling consistency\nexp_3class_masked = exp_3class_grid.clip(full_ecozone_fc)\n\n# Add the 3-class grid to the map\nexp3classMap.addLayer(exp_3class_masked, exp_3class_vis, '3-Class Exp vs Linear Fit')\n\n#  Add ecozone and national boundaries\nexp3classMap.addLayer(\n    full_ecozone_fc.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Ecozone Boundary'\n)\nexp3classMap.addLayer(\n    canada_country.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Canada Border'\n)\nexp3classMap.addLayer(\n    canada_provinces.style(**{'color': 'gray', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Province Boundaries'\n)\n\n# Add legend for 3-class interpretation\nlegend_html_3class = HTML(\n    value=\"\"\"\n    <div style='background:white; font-size:16px; padding:10px; border:none; margin-left:40px;'>\n        <b>NDVI Model Fit Classification</b><br>\n        <div><span style='background:#D3D3D3; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Linear Fit Better (0)</div>\n        <div><span style='background:#FF0000; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Exponential Fit Better (1)</div>\n        <div><span style='background:#32CD32; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Exponential Recovered Linear No Trend (2)</div>\n    </div>\n    \"\"\"\n)\nexp3classMap.add_control(WidgetControl(widget=legend_html_3class, position='bottomleft'))\n\n#  Add scale control\nexp3classMap.add_control(ScaleControl(position='bottomleft'))\n\n#  Display the map\nexp3classMap\n","metadata":{"id":"EgGwlLFq9vh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"gwJ4xR749L36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 8: Identify remaining pixels in exp_3class_grid that are still 0 (Linear better)\nstill_linear_pixels = exp_3class_grid.eq(0)\n\n# Step 9: Identify where original linear trend was non-zero\nnon_zero_linear = lineartrend_class.gt(0)\n\n# Step 10: Find pixels where linear was non-zero but exp_3class still says 0\nlinear_only_fill = still_linear_pixels.And(non_zero_linear)\n\n# Step 11: Update exp_3class_grid to assign class 3 at those positions\nexp4class = exp_3class_grid.where(linear_only_fill, 3)\n","metadata":{"id":"wi5vweS09L7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7AmEJItv9MA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new map to visualize exp4class\nexp4Map = geemap.Map(center=[60, -96], zoom=3, height=\"1000px\")\n\n# Define visualization for exp4class: 0=insignificant, 1=exp better, 2=newly captured, 3=linear trend\nexp4_vis = {\n    'min': 0,\n    'max': 3,\n    'palette': ['#D3D3D3', '#FF0000', '#FFA500', '#0000FF']  # gray, red, orange, blue\n}\n\n#  Add exp4class layer\nexp4Map.addLayer(exp4class.clip(full_ecozone_fc), exp4_vis, 'Full Model Fit Comparison (4 classes)')\n\n#  Add ecozone & national boundaries\nexp4Map.addLayer(\n    full_ecozone_fc.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Ecozone Boundary'\n)\n\ncanada_country = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\").filter(ee.Filter.eq(\"country_na\", \"Canada\"))\ncanada_provinces = ee.FeatureCollection(\"FAO/GAUL/2015/level1\").filter(ee.Filter.eq(\"ADM0_NAME\", \"Canada\"))\n\nexp4Map.addLayer(\n    canada_country.style(**{'color': 'black', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Canada Border'\n)\n\nexp4Map.addLayer(\n    canada_provinces.style(**{'color': 'gray', 'fillColor': '00000000', 'width': 1}),\n    {}, 'Province Boundaries'\n)\n\n# ðŸ“ Add scale bar\nfrom ipyleaflet import ScaleControl\nexp4Map.add_control(ScaleControl(position='bottomleft'))\n\n#  Add legend with 4 class labels\nfrom ipywidgets import HTML\nlegend_html_exp4 = HTML(\n    value=\"\"\"\n    <div style='background:white; font-size:16px; padding:10px; border:none; margin-left:40px;'>\n        <b>NDVI Model Fit (4-Class)</b><br>\n        <div><span style='background:#D3D3D3; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Insignificant (0)</div>\n        <div><span style='background:#FF0000; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Exponential Better (1)</div>\n        <div><span style='background:#FFA500; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>New Exponential Capture (2)</div>\n        <div><span style='background:#0000FF; width:20px; height:20px; display:inline-block;\n            margin-right:10px; border:1px solid black;'></span>Linear Trend (3)</div>\n    </div>\n    \"\"\"\n)\n\nfrom ipyleaflet import WidgetControl\nexp4Map.add_control(WidgetControl(widget=legend_html_exp4, position='bottomleft'))\n\n#  Show map\nexp4Map\n","metadata":{"id":"lzi8Wf2v70kY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 11: Update exp_3class_grid to assign class 3 at those positions\nexp4class = exp_3class_grid.where(linear_only_fill, 3)\n","metadata":{"id":"jTqq1XR6-Iap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 12: Assign class 5 where combined_mask == 1\nexp5class = exp4class.where(combined_mask.eq(1), 4).rename('Exp5class')\n","metadata":{"id":"nPtre6bw-Ik8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Reclassify exp5class: set to 6 where ecozone_mask == 0 (i.e., outside ecozone)\nexp5class_export_ready = exp5class.where(ecozone_mask.eq(0), 6).rename('NDVI_ModelFit_5class')\n\n# Clip to tile extent\nexp5class_clipped = exp5class_export_ready.clip(tiles_fc.geometry())\n","metadata":{"id":"v1hQEtK4-Ioj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Rename both source images to avoid conflicts\nexp_class_final_named = exp_class_final.rename('Original_Exp_Class')\nexp5class_named = exp5class.rename('Exp5_Class_Source')\n\n# Create a mask where exp5class is 1 or 2\nexp5_mask = exp5class_named.eq(1).Or(exp5class_named.eq(2))\n\n# Apply the mask: keep exp_class_final values only where exp5class is 1 or 2\nexp_class_filtered = exp_class_final_named.where(exp5_mask.Not(), 0).rename('Filtered_Exp_Class_By_Exp5')\n\n#  Visualization (optional, to confirm before export)\nvis_params = {\n    'min': 0,\n    'max': 5,\n    'palette': ['#D3D3D3', '#FF0000', '#FFA500', '#0000FF', '#00FFFF', '#008000']\n}\nMap = geemap.Map(center=[60, -96], zoom=3)\nMap.addLayer(exp_class_filtered.clip(full_ecozone_fc), vis_params, ' Filtered Exp Class by Exp5 (New)')\nMap\n","metadata":{"id":"ePffaJAa-RgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exporting exp trend class","metadata":{"id":"f1C_oYea-ZoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()  # Convert to client-side integer\n\n# Export settings\nno_data_value = 9999\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n# Loop through tiles and create export tasks\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1  # Sequential numbering from 1\n\n    description = f'exp5class_{tile_id}'\n    file_name_prefix = f'exp5class_{tile_id}'\n\n    image_to_export = exp5class_clipped.unmask(no_data_value).toInt16().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='linear_exp_combined',  # Folder name for this export\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\"All export tasks submitted. Go to the Earth Engine Code Editor 'Tasks' tab to run them manually.\")\n","metadata":{"id":"HId7cHvW-RrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exporting trend slope grid values","metadata":{"id":"cXmFwVhQ-Ru1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the slope layer and rename\nexp_slope_raw = per_pixel_exp_stack.select('Exp_Slope_Transformed')\n\n# Set slope to 0 where exp5class is 3 or 5\nexp_slope_masked_by_class = exp_slope_raw.where(\n    exp5class.eq(3).Or(exp5class.eq(5)), 0\n)\n\n# Set slope to 0 where combined_mask == 1 (e.g., invalid or water/built-up)\nexp_slope_masked_by_combined = exp_slope_masked_by_class.where(\n    combined_mask.eq(1), 0\n)\n\n# Set slope to 0 outside the tile extent\ntiles_extent = tiles_fc.geometry()\nexp_slope_clipped = exp_slope_masked_by_combined.clip(tiles_extent)\n\n# Optional: Round to 4 decimals\nexp_slope_rounded = exp_slope_clipped.multiply(10000).round().divide(10000)\n","metadata":{"id":"ttmY78-zACXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tiles again\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()  # Convert to client-side integer\n\n# Export settings\nno_data_value = 0  # We already zeroed out masked regions\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n#  Loop through tiles and export\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1\n\n    description = f'expslope_{tile_id}'\n    file_name_prefix = f'expslope_{tile_id}'\n\n    image_to_export = exp_slope_rounded.clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='EXP_SLOPE',\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\"Export task submitted for tile {tile_id}: {description}\")\n\nprint(\" All export tasks submitted. Check the Earth Engine Code Editor 'Tasks' tab to monitor progress.\")\n","metadata":{"id":"Lqz0-YFK-Ir1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"NS98IHY370oL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tkZcqfVs60UL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Load final classified image (assumed already computed)\n#  Load tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()  # Convert to client-side integer\n\n#  Export settings\nno_data_value = 9999\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n#  Loop through tiles and create export tasks\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1  # Sequential numbering from 1\n\n    description = f'linearclass_{tile_id}'\n    file_name_prefix = f'linearclass_{tile_id}'\n\n    image_to_export = final_class2_clipped.unmask(no_data_value).toInt16().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='LINEAR_TREND',  # Folder name updated\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\"All export tasks submitted. Go to the Earth Engine Code Editor 'Tasks' tab to run them manually.\")\n","metadata":{"id":"4p2Of0lJpZbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ZXqh5xe2poYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Load tiles\ntiles_fc = ee.FeatureCollection(\"projects/danielsiglab/assets/tileroi50\")\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()\n\n#  Export settings\nno_data_value = 0  # Suitable for float rasters\ncrs = 'EPSG:4326'\ncrs_transform = [0.0002695, 0, -180, 0, -0.0002695, 90]\n\n#  Loop through tiles and export slope\nfor i in range(num_tiles):\n    tile = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id = i + 1\n\n    description = f'linearslope_{tile_id}'\n    file_name_prefix = f'linearslope_{tile_id}'\n\n    image_to_export = final_static_slope_clipped.unmask(no_data_value).toFloat().clip(tile_geom)\n\n    task = ee.batch.Export.image.toDrive(\n        image=image_to_export,\n        description=description,\n        folder='linear_slope',  # Folder name for slope\n        fileNamePrefix=file_name_prefix,\n        region=tile_geom,\n        crs=crs,\n        crsTransform=crs_transform,\n        maxPixels=1e13\n    )\n\n    task.start()\n    print(f\" Export task submitted for tile {tile_id}: {description}\")\n\nprint(\" All slope export tasks submitted. Check the Earth Engine 'Tasks' tab to monitor.\")\n","metadata":{"id":"lDoiAvCopocW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xx2NIrJiJh_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From step 1-28 below is for mapping landcover type transitions into vegetation gain (greening),\n#vegetation loss (browning), stable (no chnage),lateral vegatation shift, lateral non vegetation shift\n#The code demontrate how mapped landcover transtion with VLCE2 (Lancover gridded dataset called \"COMBINEDVCGL\")\n#The final export is 30m landcover coded grid that then enabled us to intersect NDVI trend class and landcover transiiton classes\n#The Implementation was fully in Google Earth Engine Platform.","metadata":{"id":"_QEldamIJh71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"// 1. Load VLCE2 land cover\nvar ca_lc = ee.ImageCollection(\"projects/sat-io/open-datasets/CA_FOREST_LC_VLCE2\");\n\n// 2. Load ecozones and exclude ECOZONE_ID 1 and 3\nvar ecozones = ee.FeatureCollection(\"projects/danielsiglab/assets/studyarea50\");\nvar filteredEcozones = ecozones\n  .filter(ee.Filter.neq('ID', 1))\n  .filter(ee.Filter.neq('ID', 3));\nvar region = filteredEcozones.geometry();\n\n// 3. Define comparison years\nvar compareYears = ee.List.sequence(1984, 2022).getInfo();\n\n// 4. Define VLCE2 original classes and remapped 0â€“12\nvar originalClasses = [0, 20, 31, 32, 33, 40, 50, 80, 81, 100, 210, 220, 230];\nvar newSequentialClasses = ee.List.sequence(0, 12);\n\n// 5. Function to reclassify land cover\nfunction getReclassLC(year) {\n  var raw = ca_lc\n    .filter(ee.Filter.calendarRange(year, year, 'year'))\n    .first()\n    .select('b1');\n  return raw.remap(originalClasses, newSequentialClasses).rename('LC');\n}\n\n// 6. Create baseline (mode of 1984â€“1986)\nvar baseline = ee.ImageCollection.fromImages([\n  getReclassLC(1984),\n  getReclassLC(1985),\n  getReclassLC(1986)\n]).reduce(ee.Reducer.mode()).rename('Baseline');\n\n// 7. Define stable transitions: 0â†’0, ..., 12â†’12 = 1212\nvar noChangeCodes = ee.List.sequence(0, 12).map(function(x) {\n  return ee.Number(x).multiply(100).add(x);\n});\n\n// 8. Track transitions and compute change metrics\nvar changeCount = ee.Image(0).rename('Change_Count');\nvar transitionImages = [];\nvar changedClassImages = [];\n\ncompareYears.forEach(function(year) {\n  var currentLC = getReclassLC(year);\n  var transition = baseline.multiply(100).add(currentLC).rename('Transition_' + year);\n  transitionImages.push(transition);\n\n  // Visualize transition\n  Map.addLayer(transition.clip(region), {\n    min: 0,\n    max: 1212,\n    palette: ['#cccccc', '#a6cee3', '#1f78b4', '#b2df8a', '#33a02c',\n              '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6',\n              '#6a3d9a', '#ffff99', '#b15928']\n  }, 'Transition_' + year);\n\n  // Count change years\n  var changed = currentLC.neq(baseline);\n  changeCount = changeCount.add(changed);\n\n  // Mask stable pixels and rename\n  var changedClass = currentLC.updateMask(changed).rename('LC_' + year);\n  changedClassImages.push(changedClass);\n});\n\n// 9. Stack changed classes into a multiband image\nvar changedClassStack = ee.ImageCollection(changedClassImages).toBands();\n\n// 10. Count number of unique changed classes\nvar uniqueChangeCount = changedClassStack\n  .reduce(ee.Reducer.countDistinctNonNull())\n  .rename('Unique_Class_Changes');\n\n// 11. Unmask for completeness (important for full rendering!)\nvar changeCountFixed = changeCount.unmask(0).rename('Change_Count');\nvar uniqueChangeCountFixed = uniqueChangeCount.unmask(0).rename('Unique_Class_Changes');\n\n// 12. Add map layers\nMap.addLayer(baseline.clip(region), {\n  min: 0, max: 12,\n  palette: [\n    '#686868', '#3333ff', '#ccffff', '#cccccc', '#996633', '#ffccff',\n    '#ffff00', '#993399', '#9933cc', '#ccff33', '#006600', '#00cc00', '#cc9900'\n  ]\n}, 'Baseline (1984â€“1986)');\n\nMap.addLayer(changeCountFixed.clip(region), {\n  min: 0, max: 40,\n  palette: ['#ffffff', '#fee5d9', '#fcae91', '#fb6a4a', '#de2d26', '#a50f15']\n}, 'Total Change Years (Times Changed)');\n\nMap.addLayer(uniqueChangeCountFixed.clip(region), {\n min: 0, max: 5,\n palette: ['#ffffff', '#d9f0a3', '#addd8e', '#78c679', '#41ab5d', '#238443', '#005a32']\n}, 'Unique Changed Class Count');\n\n//Map.addLayer(uniqueChangeCountFixed.clip(region), {\n // min: 0, max: 30,\n // palette: [\n //   '#440154', '#482475', '#414487', '#355f8d', '#2a788e',\n //   '#21908d', '#22a884', '#43bf71', '#7ad151', '#bddf26',\n //   '#fde725'\n//  ]\n//}, 'Unique Changed Class Count (Improved)');\n\n// 13. Export results\n//Export.image.toAsset({\n//  image: changeCountFixed,\n//  description: 'Export_Change_Count_Times_wo_eco1_3',\n  //assetId: 'projects/danielsiglab/assets/change_count_times_1984_2022_wo_eco1_3',\n  //region: region,\n  //scale: 30,\n  //maxPixels: 1e13\n//});\n\n//Export.image.toAsset({\n  //image: uniqueChangeCountFixed,\n  //description: 'Export_Unique_Class_Change_Count_Fixed_wo_eco1_3',\n  //assetId: 'projects/danielsiglab/assets/unique_class_change_count_fixed_wo_eco1_3',\n  //region: region,\n  //scale: 30,\n  //maxPixels: 1e13\n//});\n\n// 14. Center the map\nMap.centerObject(region, 4);\n\n\n// Step 15: Encode only years of actual landcover transitions from previous year\nvar encodedChangeYears = ee.Image(0).rename('Encoded_Years');\nvar lastLC = baseline;\n\n// Loop over each year and encode only when the LC changes from the last year's LC\ncompareYears.forEach(function(year, index) {\n  var currentLC = getReclassLC(year);\n  var isChanged = currentLC.neq(lastLC);  // compare with previous year\n\n  // Convert index to padded 2-digit position (e.g., '01' for 1984, '39' for 2022)\n  var yearPos = ee.Number(index).add(1);\n  var paddedCode = ee.Number.parse(yearPos.format('%02d'));\n\n  // Only encode pixels that changed compared to the last year\n  encodedChangeYears = encodedChangeYears.multiply(100).add(\n    ee.Image.constant(paddedCode).updateMask(isChanged)\n  ).unmask(encodedChangeYears);\n\n  // Update lastLC for next iteration\n  lastLC = currentLC;\n});\n\n// Rename for clarity\nencodedChangeYears = encodedChangeYears.rename('Encoded_Years');\n\n// 16. Visualize Encoded Change Years\nMap.addLayer(encodedChangeYears.clip(region), {\n  min: 0,\n  max: 999999999999,  // Up to 6 transitions = 12 digits\n  palette: ['white', 'black']\n}, 'Encoded Change Years (2-digit position)');\n\n// 17. Export Encoded Change Years\nExport.image.toAsset({\n  image: encodedChangeYears,\n  description: 'Export_Encoded_Change_Years_wo_eco1_3',\n  assetId: 'projects/danielsiglab/assets/encoded_change_years_wo_eco1_3',\n  region: region,\n  scale: 30,\n  maxPixels: 1e13\n});\n\n\n//  Transition Layer Subsetting and Implementation\n// 18. Convert transitionImages array to a properly structured ImageCollection\n\nvar transitionCollection = ee.ImageCollection.fromImages(\n  transitionImages.map(function(img, index) {\n    return ee.Image(img).set('year', compareYears[index]);\n  })\n);\n\n\n//  19. Extract Transition Layers for 2020, 2021, 2022\nvar transition2020 = transitionCollection.filter(ee.Filter.eq('year', 2020)).first();\nvar transition2021 = transitionCollection.filter(ee.Filter.eq('year', 2021)).first();\nvar transition2022 = transitionCollection.filter(ee.Filter.eq('year', 2022)).first();\n\n//  20. Define Class Codes for Greening, Browning, Lateral Shifts\n//var greeningCodes = ee.List([5, 6, 7, 8, 9, 10, 11, 12, 105, 106, 107, 108, 109, 110, 111, 112,\n // 205, 206, 207, 208, 209, 210, 211, 212, 305, 306, 307, 308, 309, 310, 311, 312,\n // 405, 406, 407, 408, 409, 410, 411, 412, 510, 511, 512, 610, 611, 612, 910, 911, 912]);\n\n//var browningCodes = ee.List([500, 501, 502, 503, 504, 600, 601, 602, 603, 604,\n  //700, 701, 702, 703, 704, 800, 801, 802, 803, 804,\n  //900, 901, 902, 903, 904, 1000, 1001, 1002, 1003,\n  //1004, 1005, 1006, 1009, 1100, 1101, 1102, 1103,\n // 1104, 1105, 1106, 1109, 1200, 1201, 1202, 1203,\n//  1204, 1205, 1206, 1209]);\n\n//var lateralShiftCodes = ee.List([405, 406, 407, 408, 409, 410, 411, 412,\n//504, 506, 507, 508, 509, 605, 607, 608, 609, 705, 706, 708, 709, 710, 711,\n//712, 805, 806, 807, 809, 810, 811, 812, 905, 906, 907, 908, 1007, 1008, 1011,\n//1012, 1107, 1108, 1110, 1112, 1207, 1208, 1210, 1211]);\n\n//var lateralNonVegShiftCodes = ee.List([1, 2, 3, 4, 100, 102, 103, 104, 200, 201, 203, 204, 300, 301, 302, 304, 400, 401, 402, 403]);\n\n//var stableCodes = ee.List([0, 101, 202, 303, 404, 505, 606, 707, 808, 909, 1010, 1111, 1212]);\n\n\n// 20. Define Class Codes for Greening, Browning, Lateral Shifts\nvar greeningCodes = ee.List([5, 6, 7, 8, 9, 10, 11, 12, 105, 106, 107, 108, 109, 110, 111, 112,\n    205, 206, 207, 208, 209, 210, 211, 212, 305, 306, 307, 308, 309, 310, 311, 312,\n    405, 406, 407, 408, 409, 410, 411, 412, 506, 507, 508, 509, 510, 511, 512, 607, 610, 611, 612, 907, 910, 911, 912,1008]);\n\nvar browningCodes = ee.List([500, 501, 502, 503, 504, 600, 601, 602, 603, 604, 605,\n    700, 701, 702, 703, 704, 800, 801, 802, 803, 804,806,\n    900, 901, 902, 903, 904, 905, 1000, 1001, 1002, 1003,\n    1004, 1005, 1006, 1009, 1100, 1101, 1102, 1103,\n    1104, 1105, 1106, 1109, 1200, 1201, 1202, 1203,\n    1204, 1205, 1206, 1209]);\n\nvar lateralShiftCodes = ee.List([405, 406, 407, 408, 409, 410, 411, 412, 506, 508, 509, 608,\n609, 705, 706, 708, 709, 710, 711, 712, 805, 807, 809, 810, 811, 812, 906, 908, 1007, 1011,\n1012, 1107, 1108, 1110, 1112, 1207, 1208, 1210, 1211]);\n\nvar lateralNonVegShiftCodes = ee.List([1, 2, 3, 4, 100, 102, 103, 104, 200, 201, 203, 204, 300, 301, 302, 304, 400, 401, 402, 403]);\n\nvar stableCodes = ee.List([0, 101, 202, 303, 404, 505, 606, 707, 808, 909, 1010, 1111, 1212]);\n\n\n\n// 21. Function to Check Membership in a Code List\nfunction isInList(img, codeList) {\n  var masks = codeList.map(function(code) {\n    var codeImage = ee.Image.constant(code);\n    return img.eq(codeImage);\n  });\n  return ee.ImageCollection.fromImages(masks).max();\n}\n\nprint('isInList function defined.');\n\n// 22. Apply Mapping to Transition Collection\nvar green2020 = isInList(transition2020, greeningCodes);\nvar green2021 = isInList(transition2021, greeningCodes);\nvar green2022 = isInList(transition2022, greeningCodes);\n\nvar brown2020 = isInList(transition2020, browningCodes);\nvar brown2021 = isInList(transition2021, browningCodes);\nvar brown2022 = isInList(transition2022, browningCodes);\n\nvar lateral2020 = isInList(transition2020, lateralShiftCodes);\nvar lateral2021 = isInList(transition2021, lateralShiftCodes);\nvar lateral2022 = isInList(transition2022, lateralShiftCodes);\n\n\n\n// 23. Determine the Final Classification\n// Apply mapping for lateralNonVegShiftCodes\nvar nonVeg2020 = isInList(transition2020, lateralNonVegShiftCodes);\nvar nonVeg2021 = isInList(transition2021, lateralNonVegShiftCodes);\nvar nonVeg2022 = isInList(transition2022, lateralNonVegShiftCodes);\n\n\n// 23â€“25. Two-year consistency only (no single-year fallback)\nvar initialClass = ee.Image(0)\n  // Pass 1: must agree in 2022 & 2021\n  .where(green2022.eq(1).and(green2021.eq(1)),     1)\n  .where(brown2022.eq(1).and(brown2021.eq(1)),     2)\n  .where(lateral2022.eq(1).and(lateral2021.eq(1)), 3)\n  .where(nonVeg2022.eq(1).and(nonVeg2021.eq(1)),   4);\n\nvar finalClass = initialClass\n  // Pass 2: for any still-zero pixel, must agree in 2022 & 2020\n  .where(initialClass.eq(0)\n         .and(green2022.eq(1).and(green2020.eq(1))),     1)\n  .where(initialClass.eq(0)\n         .and(brown2022.eq(1).and(brown2020.eq(1))),     2)\n  .where(initialClass.eq(0)\n         .and(lateral2022.eq(1).and(lateral2020.eq(1))), 3)\n  .where(initialClass.eq(0)\n         .and(nonVeg2022.eq(1).and(nonVeg2020.eq(1))),   4);\n\n// Everything else remains 0 (stable)\n\n\n// 25. Set Stable Class\nfinalClass = finalClass.where(\n  finalClass.eq(0), 0\n);\n\n// 26. Visualization and Export\nMap.addLayer(finalClass.clip(region), {\n  min: 0, max: 3,\n  palette: ['#0000FF', '#00FF00', '#FF0000', '#00FFFF', '#FFA500']\n}, 'Ecological Change Classification');\n\n//Export.image.toAsset({\n // image: finalClass,\n // description: 'Ecological_Change_Class_2020_2022_Majority_2022_Focus',\n // assetId: 'projects/danielsiglab/assets/ecological_change_class_2020_2022_majority_2022_focus',\n // region: region,\n  //scale: 30,\n  //maxPixels: 1e13\n//});\n\nprint('Ecological Change Classification with 2022 Focus and Majority Logic completed.');\n\n\n// 27. Patch 2022 VLCE2 transition with stable codes where finalClass == 0\n// Compute Baselineâ†’Baseline â€œno-changeâ€ code\nvar stableVLCE = baseline.multiply(100).add(baseline);\n\n// Pull the original 2022 transition\nvar rawT2022 = transitionCollection\n  .filter(ee.Filter.eq('year', 2022))\n  .first();\n\n// Create a patched transition:\n// where finalClass â‰  0, keep rawT2022;\n//  where finalClass == 0, write stableVLCE instead.\nvar transition2022_Patched = rawT2022.where(\n  finalClass.eq(0),\n  stableVLCE\n).rename('Transition_2022_Patched');\n\n// Optional: visualize to check\nMap.addLayer(transition2022_Patched.clip(region), {\n  min: 0, max: 1212,\n  palette: [\n    '#ffffff','#cccccc','#a6cee3','#1f78b4','#b2df8a','#33a02c',\n    '#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a',\n    '#ffff99','#b15928'\n  ]\n}, 'Patched Transition 2022 VLCE2');\n\n//28 Export of landcover harmonized transition grid\n\n//  Load your tile grid and your finalClass image\n//var tilesFC    = ee.FeatureCollection(\"projects/danielsiglab/assets/tilenoneco3_74\");\n//var finalClass = finalClass;  // Make sure you've defined finalClass earlier in your script\n\n//  Define NoData, CRS & transform (matching your example)\n//var noDataVal    = 999;\n//var crs          = 'EPSG:4326';\n//var crsTransform = [0.0002695, 0, -180, 0, -0.0002695, 90];\n\n//  Convert tile collection to a client-side list\n//var tileList = tilesFC.toList(tilesFC.size());\n//var numTiles = tileList.size().getInfo();\n\n//  Loop over tiles and export finalClass clipped to each\n//for (var i = 0; i < numTiles; i++) {\n // var tile   = ee.Feature(tileList.get(i));\n  //var geom   = tile.geometry();\n  //var tileID = i + 1;  // sequential numbering\n\n  //Export.image.toDrive({\n    //image:          finalClass\n    //                 .unmask(noDataVal)\n    //                  .toInt16()   // class codes are integer\n    //                  .clip(geom),\n   // description:     'FinalClass_Tile_' + tileID,\n   // folder:          'CORRECTVCLE2_ECO_CHANGE',   // â† Your specified folder\n    //fileNamePrefix:  'final_class_tile_' + tileID,\n   // crs:             crs,\n   // crsTransform:    crsTransform,\n    //region:          geom,\n   // maxPixels:       1e13\n  //});\n\n // print('Submitted export for FinalClass tile', tileID);\n//}\n\n//print(' All FinalClass export tasks submitted.');\n","metadata":{"id":"djoi8X_HJh4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7KWDrCd7b-oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Estimating the Energy Trend from ERA5-LAND REANALYSIS","metadata":{"id":"GSIfOrDwpo7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Define study area\nstudyarea50 = full_ecozone_fc\n\n#  Define years\nstart_year = 1986\nend_year   = 2023\nyear_list  = ee.List.sequence(start_year, end_year)\n\n# Define season months (change as needed)\nsummer_months = ee.List([3, 4, 5])  # e.g., Spring = Marâ€“May; set [6,7,8,9] for Junâ€“Sep\n\n# Load ERA5-Land monthly & add 'month'\nera5land = ee.ImageCollection('ECMWF/ERA5_LAND/MONTHLY')\ndef add_month(img):\n    date = ee.Date(img.get('system:time_start'))\n    return img.set('month', date.get('month'))\nera5land_with_month = era5land.map(add_month)\n\n#  Seasonal image with H, LE, Rns, Rnl, forecast_albedo, imbalance, and Bowen ratio (H/LE)\ndef get_season_energy_image(y):\n    y = ee.Number(y)\n\n    season_ic = (era5land_with_month\n        .filterDate(ee.Date.fromYMD(y, 1, 1), ee.Date.fromYMD(y, 12, 31))\n        .filter(ee.Filter.inList('month', summer_months))        #  use your months list\n        .filterBounds(studyarea50.geometry()))                    #  robust clip target\n\n    return ee.Algorithms.If(\n        season_ic.size().eq(0),\n        None,\n        (\n            # Mean seasonal composites\n            season_ic.select([\n                'surface_sensible_heat_flux',\n                'surface_latent_heat_flux',\n                'surface_net_solar_radiation',\n                'surface_net_thermal_radiation',\n                'forecast_albedo'\n            ]).mean().rename([\n                'sensible_flux',    # H\n                'latent_flux',      # LE\n                'net_solar_rad',    # Rns\n                'net_thermal_rad',  # Rnl\n                'forecast_albedo'   # ERA5-Land albedo band\n            ])\n            # Imbalance = Rns + Rnl âˆ’ (LE + H)\n            .addBands(\n                season_ic.mean().expression(\n                    'solar + thermal - (latent + sensible)',\n                    {\n                        'solar':    season_ic.select('surface_net_solar_radiation').mean(),\n                        'thermal':  season_ic.select('surface_net_thermal_radiation').mean(),\n                        'latent':   season_ic.select('surface_latent_heat_flux').mean(),\n                        'sensible': season_ic.select('surface_sensible_heat_flux').mean()\n                    }\n                ).rename('imbalance')\n            )\n            # Bowen ratio = H / LE (mask tiny LE to avoid blow-ups)\n            .addBands(\n                ee.Image().expression(\n                    'H / (LE + 1e-9)',\n                    {\n                        'H':  season_ic.select('surface_sensible_heat_flux').mean(),\n                        'LE': season_ic.select('surface_latent_heat_flux').mean()\n                    }\n                ).updateMask(\n                    season_ic.select('surface_latent_heat_flux').mean().gt(1e-9)\n                ).rename('bowen_ratio')\n            )\n            .clip(studyarea50.geometry())\n            .set({\n                'year': y,\n                'system:time_start': ee.Date.fromYMD(y, 7, 1).millis()  # mid-year stamp for time slider/debug\n            })\n        )\n    )\n\n#  Build list â†’ ImageCollection\nall_season_energy_images        = year_list.map(get_season_energy_image)\nall_season_energy_images_clean  = all_season_energy_images.removeAll([None])\nfull_season_energy_collection   = ee.ImageCollection.fromImages(all_season_energy_images_clean)\n\nprint(' Seasonal ERA5-Land collection size:', full_season_energy_collection.size().getInfo())\nprint(' Bands sample:', full_season_energy_collection.first().bandNames().getInfo())\n","metadata":{"id":"l9YtsPWTdugB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"PC2TmffHduk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Define tiling function for the ENERGY collection\ndef split_energy_to_tiles(tile_feature):\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id   = tile_feature.get('tile_id')  # safer than getString\n\n    def clip_and_tag(img):\n        return (ee.Image(img)\n                .clip(tile_geom)\n                .set('tile_id', tile_id)\n                .set('year', img.get('year'))\n                .set('system:time_start', img.get('system:time_start')))\n\n    return full_season_energy_collection.map(clip_and_tag)\n\n#  Apply to all tiles â†’ returns List of ImageCollections\nall_tiles_energy_images = tiles_fc.map(split_energy_to_tiles)\n\n#  Flatten and wrap as ImageCollection\nenergy_tiles_collection = ee.ImageCollection(all_tiles_energy_images.flatten())\n\n#  Quick sanity checks\nprint(' Energy tiles collection ready. Count:', energy_tiles_collection.size().getInfo())\nfirst_img = energy_tiles_collection.first()\nprint('Bands in first tiled image:', first_img.bandNames().getInfo())\nprint('Sample props:', first_img.toDictionary(['tile_id','year']).getInfo())\n","metadata":{"id":"snFC3qiTdupg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ndWpfcRiOhsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Define tiling function for the ENERGY collection\ndef split_energy_to_tiles(tile_feature):\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id = tile_feature.getString('tile_id')\n\n    # Clip and tag each energy image for this tile\n    def clip_and_tag(img):\n        return ee.Image(img) \\\n            .clip(tile_geom) \\\n            .set('tile_id', tile_id) \\\n            .set('year', img.get('year')) \\\n            .set('system:time_start', img.get('system:time_start'))\n\n    #  Return as ImageCollection (just like climate)\n    return full_season_energy_collection.map(clip_and_tag)\n\n# Apply to all tiles â†’ returns List of ImageCollections\nall_tiles_energy_images = tiles_fc.map(split_energy_to_tiles)\n\n# Flatten these ImageCollections\nflattened_energy_images = all_tiles_energy_images.flatten()\n\n# Convert to ImageCollection\nenergy_tiles_collection = ee.ImageCollection(flattened_energy_images)\n\n#  Print result\nprint('Energy tiles collection ready. Count:', energy_tiles_collection.size().getInfo())\n","metadata":{"id":"lz0B1tCqOiHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"QuZKEyLYOxA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===========================\n# PER-TILE TRENDS (H, LE, Imbalance, Albedo, Bowen) + PREVIEW MAP\n# ===========================\n\n# --- Trend per tile (uses energy_tiles_collection you just built) ---\ndef compute_energy_trend_tile(tile_feature):\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id   = tile_feature.get('tile_id')  # keep original type\n\n    # this tile's time series\n    tile_col = energy_tiles_collection.filter(ee.Filter.eq('tile_id', tile_id))\n    years = ee.List(tile_col.aggregate_array('year')).distinct().sort()\n\n    # build stack [H, LE, imbalance, albedo, bowen] + relativeYear\n    def build_trend_image(y):\n        y = ee.Number(y)\n        img = ee.Image(tile_col.filter(ee.Filter.eq('year', y)).first())\n        return ee.Algorithms.If(\n            img,\n            img.addBands(\n                ee.Image.constant(y.subtract(start_year)).rename('relativeYear').toFloat()\n            ).clip(tile_geom).set('year', y),\n            None\n        )\n\n    series_ic = ee.ImageCollection.fromImages(years.map(build_trend_image)).filter(ee.Filter.notNull(['relativeYear']))\n\n    # if too few years, return masked output with correct band schema\n    out_bands = [\n        'SenSlope_sensible','SenSlope_latent','SenSlope_imbalance','SenSlope_albedo','SenSlope_bowen',\n        'Tau_sensible','Tau_latent','Tau_imbalance','Tau_albedo','Tau_bowen',\n        'Z_sensible','Z_latent','Z_imbalance','Z_albedo','Z_bowen'\n    ]\n    empty = (ee.Image.constant([0]*len(out_bands)).rename(out_bands)\n             .updateMask(ee.Image(0)).clip(tile_geom).set('tile_id', tile_id))\n\n    cond = series_ic.size().gt(5)  # require â‰¥6 years\n\n    def compute():\n        # helpers\n        def sen(var):\n            return (series_ic.select(['relativeYear', var])\n                    .reduce(ee.Reducer.sensSlope())\n                    .select('slope'))\n        def ktau(var):\n            # kendallsCorrelation -> bands: 'tau', 'p-value'\n            return (series_ic.select(['relativeYear', var])\n                    .reduce(ee.Reducer.kendallsCorrelation())\n                    .select('tau'))\n\n        # slopes\n        sen_sensible  = sen('sensible_flux').rename('SenSlope_sensible')\n        sen_latent    = sen('latent_flux').rename('SenSlope_latent')\n        sen_imbalance = sen('imbalance').rename('SenSlope_imbalance')\n        sen_albedo    = sen('forecast_albedo').rename('SenSlope_albedo')\n        sen_bowen     = sen('bowen_ratio').rename('SenSlope_bowen')\n\n        # tau\n        tau_sensible  = ktau('sensible_flux').rename('Tau_sensible')\n        tau_latent    = ktau('latent_flux').rename('Tau_latent')\n        tau_imbalance = ktau('imbalance').rename('Tau_imbalance')\n        tau_albedo    = ktau('forecast_albedo').rename('Tau_albedo')\n        tau_bowen     = ktau('bowen_ratio').rename('Tau_bowen')\n\n        # Z from tau using actual N for this tile\n        N = series_ic.size()\n        def z_of(tau_img):\n            return (tau_img.multiply(3)\n                    .multiply(N.multiply(N.subtract(1)).sqrt())\n                    .divide(ee.Number(2).multiply(N.multiply(2).add(5).sqrt())))\n        z_sensible = z_of(tau_sensible).rename('Z_sensible')\n        z_latent   = z_of(tau_latent).rename('Z_latent')\n        z_imb      = z_of(tau_imbalance).rename('Z_imbalance')\n        z_albedo   = z_of(tau_albedo).rename('Z_albedo')\n        z_bowen    = z_of(tau_bowen).rename('Z_bowen')\n\n        return (sen_sensible.addBands([\n                    sen_latent, sen_imbalance, sen_albedo, sen_bowen,\n                    tau_sensible, tau_latent, tau_imbalance, tau_albedo, tau_bowen,\n                    z_sensible, z_latent, z_imb, z_albedo, z_bowen\n                ])\n                .set('tile_id', tile_id)\n                .set('system:time_start', ee.Date.fromYMD(end_year, 7, 1).millis())\n                .clip(tile_geom))\n\n    return ee.Image(ee.Algorithms.If(cond, compute(), empty))\n\n# run trends per tile\nby_tile_energy_trends = ee.ImageCollection(tiles_fc.map(compute_energy_trend_tile))\nprint('Trend images per tile:', by_tile_energy_trends.size().getInfo())\n\n# mosaic Sen slopes\nsensible_sen_mosaic  = by_tile_energy_trends.select('SenSlope_sensible').mosaic()\nlatent_sen_mosaic    = by_tile_energy_trends.select('SenSlope_latent').mosaic()\nimbalance_sen_mosaic = by_tile_energy_trends.select('SenSlope_imbalance').mosaic()\nalbedo_sen_mosaic    = by_tile_energy_trends.select('SenSlope_albedo').mosaic()\nbowen_sen_mosaic     = by_tile_energy_trends.select('SenSlope_bowen').mosaic()\n\nprint('Slope mosaics ready')\n\n\n","metadata":{"id":"uK1bgI9ZOxLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KbJKVPxtOxST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Per-tile ENERGY Trend Fitting including IMBALANCE + ALBEDO + BOWEN\ndef compute_energy_trend_tile(tile_feature):\n    tile_feature = ee.Feature(tile_feature)\n    tile_geom = tile_feature.geometry()\n    tile_id   = tile_feature.getString('tile_id')\n\n    # Filter this tile's time series\n    tile_col = energy_tiles_collection.filter(ee.Filter.eq('tile_id', tile_id))\n\n    # Distinct, sorted years\n    years = ee.List(tile_col.aggregate_array('year')).distinct().sort()\n\n    # Build per-year stack with imbalance, relativeYear\n    def build_trend_image(y):\n        y = ee.Number(y)\n        img = ee.Image(tile_col.filter(ee.Filter.eq('year', y)).first())\n        return ee.Algorithms.If(\n            img,\n            (img.addBands(\n                 # (Re)compute imbalance: Rns + Rnl âˆ’ (LE + H)\n                 img.expression(\n                     'solar + thermal - (latent + sensible)',\n                     {\n                         'solar':    img.select('net_solar_rad'),\n                         'thermal':  img.select('net_thermal_rad'),\n                         'latent':   img.select('latent_flux'),\n                         'sensible': img.select('sensible_flux')\n                     }\n                 ).rename('imbalance')\n             )\n             .addBands(ee.Image.constant(y.subtract(start_year)).rename('relativeYear').toFloat())\n             .clip(tile_geom)\n             .set('year', y)),\n            None\n        )\n\n    series_ic = ee.ImageCollection.fromImages(years.map(build_trend_image)).filter(ee.Filter.notNull(['relativeYear']))\n\n    # If too few years, return masked output with correct band schema\n    out_bands = [\n        'SenSlope_sensible','SenSlope_latent','SenSlope_imbalance','SenSlope_albedo','SenSlope_bowen',\n        'Tau_sensible','Tau_latent','Tau_imbalance','Tau_albedo','Tau_bowen',\n        'Z_sensible','Z_latent','Z_imbalance','Z_albedo','Z_bowen'\n    ]\n    empty = (ee.Image.constant([0]*len(out_bands)).rename(out_bands)\n             .updateMask(ee.Image(0)).clip(tile_geom)\n             .set('tile_id', tile_id))\n\n    cond = series_ic.size().gt(5)  # require â‰¥6 years\n\n    def compute():\n        # Helpers\n        def sen(var):\n            return (series_ic.select(['relativeYear', var])\n                    .reduce(ee.Reducer.sensSlope())\n                    .select('slope'))\n\n        def ktau(var):\n            # kendallsCorrelation returns bands: 'tau', 'p-value'\n            return (series_ic.select(['relativeYear', var])\n                    .reduce(ee.Reducer.kendallsCorrelation())\n                    .select('tau'))\n\n        # Sen's slopes\n        sen_sensible  = sen('sensible_flux').rename('SenSlope_sensible')\n        sen_latent    = sen('latent_flux').rename('SenSlope_latent')\n        sen_imbalance = sen('imbalance').rename('SenSlope_imbalance')\n        sen_albedo    = sen('forecast_albedo').rename('SenSlope_albedo')\n        sen_bowen     = sen('bowen_ratio').rename('SenSlope_bowen')\n\n        # Kendall's tau\n        tau_sensible  = ktau('sensible_flux').rename('Tau_sensible')\n        tau_latent    = ktau('latent_flux').rename('Tau_latent')\n        tau_imbalance = ktau('imbalance').rename('Tau_imbalance')\n        tau_albedo    = ktau('forecast_albedo').rename('Tau_albedo')\n        tau_bowen     = ktau('bowen_ratio').rename('Tau_bowen')\n\n        # Z from tau (large-sample approx) using actual N for this tile\n        N = series_ic.size()\n        def z_of(tau_img):\n            return (tau_img.multiply(3)\n                    .multiply(N.multiply(N.subtract(1)).sqrt())\n                    .divide(ee.Number(2).multiply(N.multiply(2).add(5).sqrt())))\n        z_sensible = z_of(tau_sensible).rename('Z_sensible')\n        z_latent   = z_of(tau_latent).rename('Z_latent')\n        z_imb      = z_of(tau_imbalance).rename('Z_imbalance')\n        z_albedo   = z_of(tau_albedo).rename('Z_albedo')\n        z_bowen    = z_of(tau_bowen).rename('Z_bowen')\n\n        return (sen_sensible.addBands([\n                    sen_latent, sen_imbalance, sen_albedo, sen_bowen,\n                    tau_sensible, tau_latent, tau_imbalance, tau_albedo, tau_bowen,\n                    z_sensible, z_latent, z_imb, z_albedo, z_bowen\n                ])\n                .set('tile_id', tile_id)\n                .set('system:time_start', ee.Date.fromYMD(end_year, 7, 1).millis())\n                .clip(tile_geom))\n\n    return ee.Image(ee.Algorithms.If(cond, compute(), empty))\n","metadata":{"id":"spTiZnyodutK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ubQrXEF_O_c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply to all tiles\nby_tile_energy_trends = ee.ImageCollection(\n    tiles_fc.map(compute_energy_trend_tile)\n)\n\nprint('Energy trend fitting per tile complete!')\n","metadata":{"id":"tq0DgDEdO_76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"jdFf5j3sdu9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Mosaic Sen's Slopes for all variables (H, LE, Imbalance, Albedo, Bowen)\nsensible_sen_mosaic  = by_tile_energy_trends.select('SenSlope_sensible').mosaic()\nlatent_sen_mosaic    = by_tile_energy_trends.select('SenSlope_latent').mosaic()\nimbalance_sen_mosaic = by_tile_energy_trends.select('SenSlope_imbalance').mosaic()\nalbedo_sen_mosaic    = by_tile_energy_trends.select('SenSlope_albedo').mosaic()\nbowen_sen_mosaic     = by_tile_energy_trends.select('SenSlope_bowen').mosaic()\n\nprint('Energy slope mosaics ready!')\n\n#  Visualize in geemap\nMap2 = geemap.Map(center=[60, -100], zoom=4)\n\n# Flux slopes (units depend on your seasonal aggregation; keep same scale as before)\nflux_viz = {'min': -10, 'max': 10, 'palette': ['blue', 'white', 'red']}\n\n# Albedo slope (unitless per year). Adjust range if too tight/loose.\nalbedo_viz = {'min': -0.03, 'max': 0.03, 'palette': ['blue', 'white', 'red']}\n\n# Bowen ratio slope (per year). Adjust if needed.\nbowen_viz = {'min': -0.2, 'max': 0.2, 'palette': ['blue', 'white', 'red']}\n\nMap2.addLayer(sensible_sen_mosaic,  flux_viz,  'SenSlope Sensible Heat (H)')\nMap2.addLayer(latent_sen_mosaic,    flux_viz,  'SenSlope Latent Heat (LE)')\nMap2.addLayer(imbalance_sen_mosaic, flux_viz,  'SenSlope Imbalance (Rnâˆ’Hâˆ’LE)')\nMap2.addLayer(albedo_sen_mosaic,    albedo_viz,'SenSlope Albedo (forecast_albedo)')\nMap2.addLayer(bowen_sen_mosaic,     bowen_viz, 'SenSlope Bowen Ratio (H/LE)')\n\nMap2.addLayerControl()\nMap2\n","metadata":{"id":"nndbiVmndvI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"bEG479juPF8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===========================\n# SPRING â€” EXPORT (masked by Z)\n# ===========================\nz_threshold   = 0.84\nnoDataVal     = 999\nera5_transform = [0.1, 0, -180, 0, -0.1, 90]\ncrs           = 'EPSG:4326'\n\n# 1) Z mosaics\nz_sensible_mosaic  = by_tile_energy_trends.select('Z_sensible').mosaic()\nz_latent_mosaic    = by_tile_energy_trends.select('Z_latent').mosaic()\nz_imbalance_mosaic = by_tile_energy_trends.select('Z_imbalance').mosaic()\nz_albedo_mosaic    = by_tile_energy_trends.select('Z_albedo').mosaic()\nz_bowen_mosaic     = by_tile_energy_trends.select('Z_bowen').mosaic()\n\nprint(\"Spring Z-score mosaics ready!\")\n\n# 2) Mask slopes with Z-threshold (keep Float32 to preserve small values)\nmasked_sensible_sen  = sensible_sen_mosaic.updateMask(z_sensible_mosaic.abs().gte(z_threshold)).unmask(noDataVal).toFloat()\nmasked_latent_sen    = latent_sen_mosaic.updateMask(z_latent_mosaic.abs().gte(z_threshold)).unmask(noDataVal).toFloat()\nmasked_imbalance_sen = imbalance_sen_mosaic.updateMask(z_imbalance_mosaic.abs().gte(z_threshold)).unmask(noDataVal).toFloat()\nmasked_albedo_sen    = albedo_sen_mosaic.updateMask(z_albedo_mosaic.abs().gte(z_threshold)).unmask(noDataVal).toFloat()\nmasked_bowen_sen     = bowen_sen_mosaic.updateMask(z_bowen_mosaic.abs().gte(z_threshold)).unmask(noDataVal).toFloat()\n\nprint('Spring masking and NoData assignment complete!')\n\n# 3) Loop & export each tile to Drive (SPRING)\ntile_list = tiles_fc.toList(tiles_fc.size())\nnum_tiles = tile_list.size().getInfo()\nprint(' Number of tiles to export (SPRING):', num_tiles)\n\nfor i in range(num_tiles):\n    tile     = ee.Feature(tile_list.get(i))\n    tile_geom = tile.geometry()\n    tile_id   = i + 1\n    region    = tile_geom.bounds().getInfo()['coordinates']\n\n    # H (sensible)\n    task_h = ee.batch.Export.image.toDrive(\n        image=masked_sensible_sen.clip(tile_geom),\n        description=f\"SPRING_Tile_ERA5_Sensible_{tile_id}\",\n        folder='ERA5_SPRING_SENSIBLETREND',\n        fileNamePrefix=f\"SPRING_Tile_ERA5_Sensible_{tile_id}\",\n        crs=crs,\n        crsTransform=era5_transform,\n        region=region,\n        maxPixels=1e13\n    ); task_h.start()\n\n    # LE (latent)\n    task_le = ee.batch.Export.image.toDrive(\n        image=masked_latent_sen.clip(tile_geom),\n        description=f\"SPRING_Tile_ERA5_Latent_{tile_id}\",\n        folder='ERA5_SPRING_LATENTTREND',\n        fileNamePrefix=f\"SPRING_Tile_ERA5_Latent_{tile_id}\",\n        crs=crs,\n        crsTransform=era5_transform,\n        region=region,\n        maxPixels=1e13\n    ); task_le.start()\n\n    # Imbalance\n    task_imb = ee.batch.Export.image.toDrive(\n        image=masked_imbalance_sen.clip(tile_geom),\n        description=f\"SPRING_Tile_ERA5_Imbalance_{tile_id}\",\n        folder='ERA5_SPRING_IMBALANCETREND',\n        fileNamePrefix=f\"SPRING_Tile_ERA5_Imbalance_{tile_id}\",\n        crs=crs,\n        crsTransform=era5_transform,\n        region=region,\n        maxPixels=1e13\n    ); task_imb.start()\n\n    # Albedo (forecast_albedo)\n    task_alb = ee.batch.Export.image.toDrive(\n        image=masked_albedo_sen.clip(tile_geom),\n        description=f\"SPRING_Tile_ERA5_Albedo_{tile_id}\",\n        folder='ERA5_SPRING_ALBEDOTREND',\n        fileNamePrefix=f\"SPRING_Tile_ERA5_Albedo_{tile_id}\",\n        crs=crs,\n        crsTransform=era5_transform,\n        region=region,\n        maxPixels=1e13\n    ); task_alb.start()\n\n    # Bowen ratio (H/LE)\n    task_br = ee.batch.Export.image.toDrive(\n        image=masked_bowen_sen.clip(tile_geom),\n        description=f\"SPRING_Tile_ERA5_Bowen_{tile_id}\",\n        folder='ERA5_SPRING_BOWENTREND',\n        fileNamePrefix=f\"SPRING_Tile_ERA5_Bowen_{tile_id}\",\n        crs=crs,\n        crsTransform=era5_transform,\n        region=region,\n        maxPixels=1e13\n    ); task_br.start()\n\n    print(f\" Spring export tasks started for tile {tile_id}\")\n\nprint('All Spring ERA5-Land export tasks submitted!')\n","metadata":{"id":"P9NO1JAAPGHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"1UENp-WXPGSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Energy Flux Trend Computations and decomposition","metadata":{"id":"gjT_K2g1d1Qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os\nfrom arcpy.sa import Raster, ExtractByMask, SetNull, Abs\n\narcpy.CheckOutExtension(\"Spatial\")\narcpy.env.overwriteOutput = True\n\ngdb = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\narcpy.env.workspace = gdb\n\n# Mask to use (INSIDE area of this dataset is kept).\n# Works if it's a raster or a polygon feature class.\nmask_ds = os.path.join(gdb, \"StudyArea_by1deglat_ras\")\n\n# Keep alignment consistent\narcpy.env.snapRaster = mask_ds\n\n# Only these three rasters, taken directly by their exact names from the GDB\ntargets = [\"spring_sens_wm2\", \"spring_latent_wm2\", \"spring_imb_wm2\"]\n\n# Value to convert to NoData (with small tolerance for float equality)\nval = 0.003858\ntol = 1e-06\n\nfor name in targets:\n    in_raster = Raster(os.path.join(gdb, name))\n\n    # 1) Extract by Mask (INSIDE area of mask_ds)\n    masked = ExtractByMask(in_raster, mask_ds)\n    tmp = os.path.join(\"in_memory\", f\"{name}_masked\")\n    masked.save(tmp)\n\n    # 2) Set cells equal to 0.003858 to NoData\n    cleaned = SetNull(Abs(Raster(tmp) - val) <= tol, Raster(tmp))\n    out = os.path.join(gdb, f\"{name}_clean\")\n    cleaned.save(out)\n\n    print(f\"Done: {name} â†’ {os.path.basename(out)}\")\n\nprint(\"Finished all three rasters.\")\n","metadata":{"id":"JUxtU_ZveI-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"anjRLZw3fCG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, math\nfrom arcpy.sa import ZonalStatisticsAsTable\n\narcpy.CheckOutExtension(\"Spatial\")\narcpy.env.overwriteOutput = True\n\n# --- Paths ---\ngdb = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\narcpy.env.workspace = gdb\n\n# Feature class with zones\nlatbands_fc = r\"U:\\\\class_transition\\landcover_transition\\landcover_transition.gdb\\Study_tse\"\n\n# Choose your zone field: \"Lat_band\" or \"Value\"\nzone_field = \"Name\"   # <-- using 'Value' as requested\n\n# Input rasters\nspring_rasters = [\n    \"fal_imb_wm2_clean\",\n    \"fal_sens_wm2_clean\",\n    \"fal_latent_wm2_clean\"\n]\n\n# Env: use native raster cell size\narcpy.env.cellSize = 0.1\n\n# --- Constants for pixel area calculation ---\nR = 6371008.7714  # meters\ndeg2rad = math.pi / 180.0\ndlat_deg = 0.1\ndlon_deg = 0.1\ndlon_rad = dlon_deg * deg2rad\n\ndef mid_lat_from_band(band_text):\n    \"\"\" Parse '44â€“45N' -> 44.5. \"\"\"\n    s = band_text.replace(\"â€“\", \"-\").replace(\"N\", \"\").strip()\n    a, b = s.split(\"-\")\n    return (float(a) + float(b)) / 2.0\n\ndef pixel_area_m2_at_lat(lat_deg):\n    \"\"\" Exact spherical formula for a 0.1Â° x 0.1Â° cell at given latitude. \"\"\"\n    phi1 = (lat_deg - dlat_deg/2.0) * deg2rad\n    phi2 = (lat_deg + dlat_deg/2.0) * deg2rad\n    return (R**2) * dlon_rad * (math.sin(phi2) - math.sin(phi1))\n\n# --- Precompute centroid latitudes for zones (for any non-Lat_band field) ---\necozone_lat = {}\nif zone_field != \"Lat_band\":\n    with arcpy.da.SearchCursor(latbands_fc, [zone_field, \"SHAPE@XY\"]) as sc:\n        for zid, (x, y) in sc:\n            ecozone_lat[zid] = y  # centroid latitude in degrees\n\n# Where to write Excel / CSV (fixed raw string)\nexport_dir = r\"X:\\\\\"\n\n# --- Main loop ---\nfor rname in spring_rasters:\n    in_ras = os.path.join(gdb, rname)\n\n    # Distinguish output table name\n    if zone_field == \"Lat_band\":\n        out_tbl = os.path.join(gdb, f\"{rname}_zstats_latbands\")\n    else:\n        out_tbl = os.path.join(gdb, f\"eco_{rname}_zstats\")\n\n    # --- Run Zonal Statistics ---\n    ZonalStatisticsAsTable(\n        in_zone_data=latbands_fc,\n        zone_field=zone_field,\n        in_value_raster=in_ras,\n        out_table=out_tbl,\n        ignore_nodata=\"DATA\",\n        statistics_type=\"ALL\"\n    )\n\n    # --- Add area fields ---\n    add_fields = [\n        (\"LAT_MID\", \"DOUBLE\"),\n        (\"PIX_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_KM2\", \"DOUBLE\")\n    ]\n    existing = {f.name for f in arcpy.ListFields(out_tbl)}\n    for fname, ftype in add_fields:\n        if fname not in existing:\n            arcpy.management.AddField(out_tbl, fname, ftype)\n\n    # --- Update areas ---\n    with arcpy.da.UpdateCursor(\n        out_tbl, [zone_field, \"COUNT\", \"LAT_MID\", \"PIX_AREA_M2\", \"VALID_AREA_M2\", \"VALID_AREA_KM2\"]\n    ) as cur:\n        for zname, cnt, lat_mid, pix_m2, area_m2, area_km2 in cur:\n            if zone_field == \"Lat_band\":\n                lat_mid = mid_lat_from_band(zname)\n            else:\n                lat_mid = ecozone_lat.get(zname, 0.0)\n\n            pix_m2  = pixel_area_m2_at_lat(lat_mid)\n            area_m2 = (cnt or 0) * pix_m2\n            area_km2 = area_m2 / 1e6\n            cur.updateRow((zname, cnt, lat_mid, pix_m2, area_m2, area_km2))\n\n    # --- Exports ---\n    base = os.path.splitext(os.path.basename(out_tbl))[0]\n\n    # Excel export\n    xlsx_path = os.path.join(export_dir, f\"{base}.xlsx\")\n    arcpy.conversion.TableToExcel(\n        Input_Table=out_tbl,\n        Output_Excel_File=xlsx_path,\n        Use_field_alias_as_column_header=\"NAME\",\n        Use_domain_and_subtype_description=\"CODE\"\n    )\n\n    # CSV export (optional â€” keep if you want .csv as well)\n    csv_path = os.path.join(export_dir, f\"{base}.csv\")\n    arcpy.conversion.TableToTable(\n        in_rows=out_tbl,\n        out_path=export_dir,\n        out_name=f\"{base}.csv\"\n    )\n\n    print(f\" Done: {os.path.basename(out_tbl)}  |  XLSX: {xlsx_path}  |  CSV: {csv_path}\")\n\nprint(\" All zonal tables created with VALID_AREA_M2/KM2 based on 0.1Â° pixels and exported to Excel/CSV.\")\n","metadata":{"id":"DTWqcp_NfCDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"iYOaE-EtfhwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using shapefile for the ecozones","metadata":{"id":"xMMYIsMpfhsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, math\nfrom arcpy.sa import ZonalStatisticsAsTable\n\narcpy.CheckOutExtension(\"Spatial\")\narcpy.env.overwriteOutput = True\n\n# --- Paths ---\ngdb = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\narcpy.env.workspace = gdb\n\n# Feature class with zones\nlatbands_fc = r\"U:\\\\class_transition\\landcover_transition\\landcover_transition.gdb\\Study_tse\"\n\n# Choose your zone field: \"Lat_band\" or \"Value\"\nzone_field = \"Name\"   # <-- using 'Value' as requested\n\n# Input rasters\nspring_rasters = [\n    \"fal_imb_wm2_clean\",\n    \"fal_sens_wm2_clean\",\n    \"fal_latent_wm2_clean\"\n]\n\n# Env: use native raster cell size\narcpy.env.cellSize = 0.1\n\n# --- Constants for pixel area calculation ---\nR = 6371008.7714  # meters\ndeg2rad = math.pi / 180.0\ndlat_deg = 0.1\ndlon_deg = 0.1\ndlon_rad = dlon_deg * deg2rad\n\ndef mid_lat_from_band(band_text):\n    \"\"\" Parse '44â€“45N' -> 44.5. \"\"\"\n    s = band_text.replace(\"â€“\", \"-\").replace(\"N\", \"\").strip()\n    a, b = s.split(\"-\")\n    return (float(a) + float(b)) / 2.0\n\ndef pixel_area_m2_at_lat(lat_deg):\n    \"\"\" Exact spherical formula for a 0.1Â° x 0.1Â° cell at given latitude. \"\"\"\n    phi1 = (lat_deg - dlat_deg/2.0) * deg2rad\n    phi2 = (lat_deg + dlat_deg/2.0) * deg2rad\n    return (R**2) * dlon_rad * (math.sin(phi2) - math.sin(phi1))\n\n# --- Precompute centroid latitudes for zones (for any non-Lat_band field) ---\necozone_lat = {}\nif zone_field != \"Lat_band\":\n    with arcpy.da.SearchCursor(latbands_fc, [zone_field, \"SHAPE@XY\"]) as sc:\n        for zid, (x, y) in sc:\n            ecozone_lat[zid] = y  # centroid latitude in degrees\n\n# Where to write Excel / CSV (fixed raw string)\nexport_dir = r\"X:\\\\\"\n\n# --- Main loop ---\nfor rname in spring_rasters:\n    in_ras = os.path.join(gdb, rname)\n\n    # Distinguish output table name\n    if zone_field == \"Lat_band\":\n        out_tbl = os.path.join(gdb, f\"{rname}_zstats_latbands\")\n    else:\n        out_tbl = os.path.join(gdb, f\"eco_{rname}_zstats\")\n\n    # --- Run Zonal Statistics ---\n    ZonalStatisticsAsTable(\n        in_zone_data=latbands_fc,\n        zone_field=zone_field,\n        in_value_raster=in_ras,\n        out_table=out_tbl,\n        ignore_nodata=\"DATA\",\n        statistics_type=\"ALL\"\n    )\n\n    # --- Add area fields ---\n    add_fields = [\n        (\"LAT_MID\", \"DOUBLE\"),\n        (\"PIX_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_KM2\", \"DOUBLE\")\n    ]\n    existing = {f.name for f in arcpy.ListFields(out_tbl)}\n    for fname, ftype in add_fields:\n        if fname not in existing:\n            arcpy.management.AddField(out_tbl, fname, ftype)\n\n    # --- Update areas ---\n    with arcpy.da.UpdateCursor(\n        out_tbl, [zone_field, \"COUNT\", \"LAT_MID\", \"PIX_AREA_M2\", \"VALID_AREA_M2\", \"VALID_AREA_KM2\"]\n    ) as cur:\n        for zname, cnt, lat_mid, pix_m2, area_m2, area_km2 in cur:\n            if zone_field == \"Lat_band\":\n                lat_mid = mid_lat_from_band(zname)\n            else:\n                lat_mid = ecozone_lat.get(zname, 0.0)\n\n            pix_m2  = pixel_area_m2_at_lat(lat_mid)\n            area_m2 = (cnt or 0) * pix_m2\n            area_km2 = area_m2 / 1e6\n            cur.updateRow((zname, cnt, lat_mid, pix_m2, area_m2, area_km2))\n\n    # --- Exports ---\n    base = os.path.splitext(os.path.basename(out_tbl))[0]\n\n    # Excel export\n    xlsx_path = os.path.join(export_dir, f\"{base}.xlsx\")\n    arcpy.conversion.TableToExcel(\n        Input_Table=out_tbl,\n        Output_Excel_File=xlsx_path,\n        Use_field_alias_as_column_header=\"NAME\",\n        Use_domain_and_subtype_description=\"CODE\"\n    )\n\n    # CSV export (optional â€” keep if you want .csv as well)\n    csv_path = os.path.join(export_dir, f\"{base}.csv\")\n    arcpy.conversion.TableToTable(\n        in_rows=out_tbl,\n        out_path=export_dir,\n        out_name=f\"{base}.csv\"\n    )\n\n    print(f\"Done: {os.path.basename(out_tbl)}  |  XLSX: {xlsx_path}  |  CSV: {csv_path}\")\n\nprint(\"All zonal tables created with VALID_AREA_M2/KM2 based on 0.1Â° pixels and exported to Excel/CSV.\")\n","metadata":{"id":"gZsvsRF-kO0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Y6ANVMXXkOwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#POST-PROCESSING IN ARGISPRO-PYTHON API","metadata":{"id":"rCapp2w9kOtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Depending on the computional memory, the user can run them once as seen below or one by one as demostrated in the next two cells","metadata":{"id":"F4ECIFgsnYFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, math\nfrom arcpy.sa import ZonalStatisticsAsTable, Con, Raster\n\narcpy.CheckOutExtension(\"Spatial\")\narcpy.env.overwriteOutput = True\n\n# --- Paths ---\ngdb = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\narcpy.env.workspace = gdb\n\n# Feature class with zones\nlatbands_fc = r\"U:\\\\class_transition\\landcover_transition\\landcover_transition.gdb\\Study_tse\"\n\n# Zone field\nzone_field = \"Name\"\n\n# Original input rasters (Fall example here)\ninput_rasters = {\n    \"wint\": \"wint_imb_wm2_clean\",\n    \"wint\": \"wint_sens_wm2_clean\",\n    \"wint\": \"wint_latent_wm2_clean\"\n}\n\n# Env: use native raster cell size\narcpy.env.cellSize = 0.1\n\n# --- Constants for pixel area calculation ---\nR = 6371008.7714  # meters\ndeg2rad = math.pi / 180.0\ndlat_deg = 0.1\ndlon_deg = 0.1\ndlon_rad = dlon_deg * deg2rad\n\ndef mid_lat_from_band(band_text):\n    \"\"\" Parse '44â€“45N' -> 44.5. \"\"\"\n    s = band_text.replace(\"â€“\", \"-\").replace(\"N\", \"\").strip()\n    a, b = s.split(\"-\")\n    return (float(a) + float(b)) / 2.0\n\ndef pixel_area_m2_at_lat(lat_deg):\n    \"\"\" Exact spherical formula for a 0.1Â° x 0.1Â° cell at given latitude. \"\"\"\n    phi1 = (lat_deg - dlat_deg/2.0) * deg2rad\n    phi2 = (lat_deg + dlat_deg/2.0) * deg2rad\n    return (R**2) * dlon_rad * (math.sin(phi2) - math.sin(phi1))\n\n# --- Precompute centroid latitudes for zones ---\necozone_lat = {}\nif zone_field != \"Lat_band\":\n    with arcpy.da.SearchCursor(latbands_fc, [zone_field, \"SHAPE@XY\"]) as sc:\n        for zid, (x, y) in sc:\n            ecozone_lat[zid] = y  # centroid latitude\n\n# Where to write Excel / CSV\nexport_dir = r\"X:\\\\\"\n\n# --- Step 1: Generate cooling/warming rasters ---\nderived_rasters = []\n\n# Latent (special sign flip)\nlat = Raster(os.path.join(gdb, input_rasters[\"latent\"]))\nlatent_cooling = Con(lat > 0, -lat, 0)\nlatent_warming = Con(lat < 0, -lat, 0)\nlatent_cooling_name = \"wint_latent_cooling\"\nlatent_warming_name = \"wint_latent_warming\"\nlatent_cooling.save(os.path.join(gdb, latent_cooling_name))\nlatent_warming.save(os.path.join(gdb, latent_warming_name))\nderived_rasters += [latent_cooling_name, latent_warming_name]\n\n# Sensible\nsens = Raster(os.path.join(gdb, input_rasters[\"sens\"]))\nsens_cooling = Con(sens < 0, sens, 0)\nsens_warming = Con(sens > 0, sens, 0)\nsens_cooling_name = \"wint_sens_cooling\"\nsens_warming_name = \"wint_sens_warming\"\nsens_cooling.save(os.path.join(gdb, sens_cooling_name))\nsens_warming.save(os.path.join(gdb, sens_warming_name))\nderived_rasters += [sens_cooling_name, sens_warming_name]\n\n# Imbalance (Ground Heat Fluxes)\nimb = Raster(os.path.join(gdb, input_rasters[\"imb\"]))\nimb_cooling = Con(imb < 0, imb, 0)\nimb_warming = Con(imb > 0, imb, 0)\nimb_cooling_name = \"wint_imb_cooling\"\nimb_warming_name = \"wint_imb_warming\"\nimb_cooling.save(os.path.join(gdb, imb_cooling_name))\nimb_warming.save(os.path.join(gdb, imb_warming_name))\nderived_rasters += [imb_cooling_name, imb_warming_name]\n\nprint(f\"Derived rasters created: {derived_rasters}\")\n\n\n# --- Step 2: Loop over derived rasters for ZonalStats ---\nfor rname in derived_rasters:\n    in_ras = os.path.join(gdb, rname)\n\n    # Distinguish output table name\n    if zone_field == \"Lat_band\":\n        out_tbl = os.path.join(gdb, f\"{rname}_zstats_latbands\")\n    else:\n        out_tbl = os.path.join(gdb, f\"eco_{rname}_zstats\")\n\n    # --- Run Zonal Statistics ---\n    ZonalStatisticsAsTable(\n        in_zone_data=latbands_fc,\n        zone_field=zone_field,\n        in_value_raster=in_ras,\n        out_table=out_tbl,\n        ignore_nodata=\"DATA\",\n        statistics_type=\"ALL\"\n    )\n\n    # --- Add area fields ---\n    add_fields = [\n        (\"LAT_MID\", \"DOUBLE\"),\n        (\"PIX_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_KM2\", \"DOUBLE\")\n    ]\n    existing = {f.name for f in arcpy.ListFields(out_tbl)}\n    for fname, ftype in add_fields:\n        if fname not in existing:\n            arcpy.management.AddField(out_tbl, fname, ftype)\n\n    # --- Update areas ---\n    with arcpy.da.UpdateCursor(\n        out_tbl, [zone_field, \"COUNT\", \"LAT_MID\", \"PIX_AREA_M2\", \"VALID_AREA_M2\", \"VALID_AREA_KM2\"]\n    ) as cur:\n        for zname, cnt, lat_mid, pix_m2, area_m2, area_km2 in cur:\n            if zone_field == \"Lat_band\":\n                lat_mid = mid_lat_from_band(zname)\n            else:\n                lat_mid = ecozone_lat.get(zname, 0.0)\n\n            pix_m2  = pixel_area_m2_at_lat(lat_mid)\n            area_m2 = (cnt or 0) * pix_m2\n            area_km2 = area_m2 / 1e6\n            cur.updateRow((zname, cnt, lat_mid, pix_m2, area_m2, area_km2))\n\n    # --- Exports ---\n    base = os.path.splitext(os.path.basename(out_tbl))[0]\n\n    xlsx_path = os.path.join(export_dir, f\"{base}.xlsx\")\n    arcpy.conversion.TableToExcel(\n        Input_Table=out_tbl,\n        Output_Excel_File=xlsx_path,\n        Use_field_alias_as_column_header=\"NAME\",\n        Use_domain_and_subtype_description=\"CODE\"\n    )\n\n    csv_path = os.path.join(export_dir, f\"{base}.csv\")\n    arcpy.conversion.TableToTable(\n        in_rows=out_tbl,\n        out_path=export_dir,\n        out_name=f\"{base}.csv\"\n    )\n\n    print(f\" Done: {os.path.basename(out_tbl)}  |  XLSX: {xlsx_path}  |  CSV: {csv_path}\")\n\nprint(\"ðŸŽ‰ All zonal tables (cooling & warming separated) created and exported.\")\n","metadata":{"id":"kKoUORHwnYQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"o0OJtVhCqNYc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The user can rerun them one by one if running all crash due to thier computation memory","metadata":{"id":"_FKjuT0DqNQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, math\nfrom arcpy.sa import ZonalStatisticsAsTable, Con, Raster\n\narcpy.CheckOutExtension(\"Spatial\")\narcpy.env.overwriteOutput = True\n\n# --- Paths ---\ngdb = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\narcpy.env.workspace = gdb\n\n# Feature class with zones\nlatbands_fc = r\"U:\\\\class_transition\\landcover_transition\\landcover_transition.gdb\\Study_tse\"\n\n# Zone field\nzone_field = \"Name\"\n\n# Only Imbalance raster\nimb_raster = \"sum_imb_wm2_clean\"\n\n# Env: use native raster cell size\narcpy.env.cellSize = 0.1\n\n# --- Constants for pixel area calculation ---\nR = 6371008.7714  # meters\ndeg2rad = math.pi / 180.0\ndlat_deg = 0.1\ndlon_deg = 0.1\ndlon_rad = dlon_deg * deg2rad\n\ndef mid_lat_from_band(band_text):\n    \"\"\" Parse '44â€“45N' -> 44.5. \"\"\"\n    s = band_text.replace(\"â€“\", \"-\").replace(\"N\", \"\").strip()\n    a, b = s.split(\"-\")\n    return (float(a) + float(b)) / 2.0\n\ndef pixel_area_m2_at_lat(lat_deg):\n    \"\"\" Exact spherical formula for a 0.1Â° x 0.1Â° cell at given latitude. \"\"\"\n    phi1 = (lat_deg - dlat_deg/2.0) * deg2rad\n    phi2 = (lat_deg + dlat_deg/2.0) * deg2rad\n    return (R**2) * dlon_rad * (math.sin(phi2) - math.sin(phi1))\n\n# --- Precompute centroid latitudes for zones ---\necozone_lat = {}\nif zone_field != \"Lat_band\":\n    with arcpy.da.SearchCursor(latbands_fc, [zone_field, \"SHAPE@XY\"]) as sc:\n        for zid, (x, y) in sc:\n            ecozone_lat[zid] = y  # centroid latitude\n\n# Where to write Excel / CSV\nexport_dir = r\"X:\\\\\"\n\n# --- Step 1: Generate cooling/warming rasters for Imbalance ---\nimb = Raster(os.path.join(gdb, imb_raster))\n#imb_cooling = Con(imb < 0, imb, 0)\nimb_warming = Con(imb > 0, imb, 0)\n\n#imb_cooling_name = \"sum_imb_cooling\"\nimb_warming_name = \"sum_imb_warming\"\n\n#imb_cooling.save(imb_cooling_name)   # save directly into gdb\nimb_warming.save(imb_warming_name)\n\n#derived_rasters = [imb_cooling_name, imb_warming_name]\nderived_rasters = [imb_warming_name]\nprint(f\" Derived rasters created: {derived_rasters}\")\n\n# --- Step 2: Loop over derived rasters for ZonalStats ---\nfor rname in derived_rasters:\n    in_ras = rname  # workspace already set to gdb\n\n    # Distinguish output table name\n    if zone_field == \"Lat_band\":\n        out_tbl = f\"{rname}_zstats_latbands\"\n    else:\n        out_tbl = f\"eco_{rname}_zstats\"\n\n    # --- Run Zonal Statistics ---\n    ZonalStatisticsAsTable(\n        in_zone_data=latbands_fc,\n        zone_field=zone_field,\n        in_value_raster=in_ras,\n        out_table=out_tbl,\n        ignore_nodata=\"DATA\",\n        statistics_type=\"ALL\"\n    )\n\n    # --- Add area fields ---\n    add_fields = [\n        (\"LAT_MID\", \"DOUBLE\"),\n        (\"PIX_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_M2\", \"DOUBLE\"),\n        (\"VALID_AREA_KM2\", \"DOUBLE\")\n    ]\n    existing = {f.name for f in arcpy.ListFields(out_tbl)}\n    for fname, ftype in add_fields:\n        if fname not in existing:\n            arcpy.management.AddField(out_tbl, fname, ftype)\n\n    # --- Update areas ---\n    with arcpy.da.UpdateCursor(\n        out_tbl, [zone_field, \"COUNT\", \"LAT_MID\", \"PIX_AREA_M2\", \"VALID_AREA_M2\", \"VALID_AREA_KM2\"]\n    ) as cur:\n        for zname, cnt, lat_mid, pix_m2, area_m2, area_km2 in cur:\n            if zone_field == \"Lat_band\":\n                lat_mid = mid_lat_from_band(zname)\n            else:\n                lat_mid = ecozone_lat.get(zname, 0.0)\n\n            pix_m2  = pixel_area_m2_at_lat(lat_mid)\n            area_m2 = (cnt or 0) * pix_m2\n            area_km2 = area_m2 / 1e6\n            cur.updateRow((zname, cnt, lat_mid, pix_m2, area_m2, area_km2))\n\n    # --- Exports ---\n    base = os.path.splitext(os.path.basename(out_tbl))[0]\n\n    xlsx_path = os.path.join(export_dir, f\"{base}.xlsx\")\n    arcpy.conversion.TableToExcel(\n        Input_Table=out_tbl,\n        Output_Excel_File=xlsx_path,\n        Use_field_alias_as_column_header=\"NAME\",\n        Use_domain_and_subtype_description=\"CODE\"\n    )\n\n    csv_path = os.path.join(export_dir, f\"{base}.csv\")\n    arcpy.conversion.TableToTable(\n        in_rows=out_tbl,\n        out_path=export_dir,\n        out_name=f\"{base}.csv\"\n    )\n\n    print(f\"Done: {out_tbl}  |  XLSX: {xlsx_path}  |  CSV: {csv_path}\")\n\nprint(\" Imbalance warming zonal tables created and exported.\")\n","metadata":{"id":"lEviMAxKl4LI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from the export (net exchange trend grid, i.e, cooling and warming signed sumed)\n#generated from above step(s), we then decompose them per ecozone. See decomposition steps below","metadata":{"id":"n6lZXMevfB_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2E5lCGBjixYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, csv\nfrom arcpy.sa import Raster, IsNull, Con, ZonalStatisticsAsTable\narcpy.CheckOutExtension(\"Spatial\")\n\n# ===================== EDIT THESE PATHS/NAMES =====================\nGDB         = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\nZONE_RAS    = \"Study_tse_Raster\"      # integer zone raster with values 1..9\nZONE_FIELD  = \"Value\"                 # field name that stores zone IDs ('Value'/'VALUE')\nNET_RASTER  = \"Net_exchange2\"         # + = warming, - = cooling, NoData = none significant\nOUT_CSV     = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\net2_zone_summary.csv\"\nOUT_TOTALS  = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\net2_grand_totals.csv\"\n\n# Optional pretty names for your 1..9 codes:\neco_name = {1:\"SA\",2:\"TP\",3:\"BS\",4:\"BC\",5:\"TC\",6:\"HP\",7:\"TSW\",8:\"TCO\",9:\"TSE\"}\n\n# ===================== ENVIRONMENT =====================\narcpy.env.workspace       = GDB\narcpy.env.overwriteOutput = True\n# Align to zone raster so counts/areas line up exactly with ecozone footprint\narcpy.env.snapRaster      = ZONE_RAS\narcpy.env.extent          = arcpy.Describe(ZONE_RAS).extent\narcpy.env.cellSize        = arcpy.Describe(ZONE_RAS).meanCellWidth\n\n# Helper to find a field name robustly (case differences, *_SUM variants, etc.)\ndef _find_field(tbl, target_upper, fallback_suffix=None):\n    fields = [f.name for f in arcpy.ListFields(tbl)]\n    for nm in fields:\n        if nm.upper() == target_upper:\n            return nm\n    if fallback_suffix:\n        for nm in fields:\n            if nm.upper().endswith(fallback_suffix):\n                return nm\n    return None\n\n# ===================== 1) TOTAL PIXELS PER ZONE (no RAT needed) =====================\n# Build a ones raster wherever the zone raster has data; sum ones per zone -> total pixel counts\nzone = Raster(ZONE_RAS)\nones = Con(IsNull(zone), 0, 1)\n\ntot_tbl = os.path.join(GDB, \"ZS_zone_totalpix\")\nif arcpy.Exists(tot_tbl):\n    arcpy.Delete_management(tot_tbl)\nZonalStatisticsAsTable(ZONE_RAS, ZONE_FIELD, ones, tot_tbl, \"DATA\", \"SUM\")\n\nsum_field = _find_field(tot_tbl, \"SUM\", fallback_suffix=\"_SUM\")\nif not sum_field:\n    raise RuntimeError(\"Could not find a SUM field in total-pixel table.\")\n\ntotal_pixels = {}\nwith arcpy.da.SearchCursor(tot_tbl, [ZONE_FIELD, sum_field]) as cur:\n    for zid, s in cur:\n        total_pixels[int(zid)] = int(round(s or 0))\n\n# ===================== 2) MASKS & VALUE RASTERS FROM NET =====================\nnet     = Raster(NET_RASTER)\nvalid01 = Con(IsNull(net), 0, 1)  # 1 where Net has data\nwarm01  = Con(net > 0, 1, 0)      # 1 where Net > 0 (warming)\ncool01  = Con(net < 0, 1, 0)      # 1 where Net < 0 (cooling)\n\n# For sums of values by sign\nwarm_vals = Con(net > 0, net, 0)  # keep positive values, 0 elsewhere\ncool_vals = Con(net < 0, net, 0)  # keep negative values, 0 elsewhere\n\ndef zonal_sum_to_gdb(value_raster, name):\n    \"\"\"Return dict {zone_id: SUM(value_raster)} written via ZonalStatisticsAsTable.\"\"\"\n    tbl = os.path.join(GDB, name)\n    if arcpy.Exists(tbl):\n        arcpy.Delete_management(tbl)\n    ZonalStatisticsAsTable(ZONE_RAS, ZONE_FIELD, value_raster, tbl, \"DATA\", \"SUM\")\n    if not arcpy.Exists(tbl):\n        raise RuntimeError(f\"ZonalStatisticsAsTable failed for {name}:\\n{arcpy.GetMessages(2)}\")\n    sfield = _find_field(tbl, \"SUM\", fallback_suffix=\"_SUM\")\n    if not sfield:\n        raise RuntimeError(f\"SUM field not found in {tbl}. Fields: {[f.name for f in arcpy.ListFields(tbl)]}\")\n    out = {}\n    with arcpy.da.SearchCursor(tbl, [ZONE_FIELD, sfield]) as cur:\n        for zid, s in cur:\n            out[int(zid)] = float(s or 0.0)\n    return out\n\ndef zonal_mean_to_gdb(value_raster, name):\n    \"\"\"Return dict {zone_id: MEAN(value_raster)} (NoData excluded).\"\"\"\n    tbl = os.path.join(GDB, name)\n    if arcpy.Exists(tbl):\n        arcpy.Delete_management(tbl)\n    ZonalStatisticsAsTable(ZONE_RAS, ZONE_FIELD, value_raster, tbl, \"DATA\", \"MEAN\")\n    if not arcpy.Exists(tbl):\n        raise RuntimeError(f\"ZonalStatisticsAsTable failed for {name}:\\n{arcpy.GetMessages(2)}\")\n    out = {}\n    with arcpy.da.SearchCursor(tbl, [ZONE_FIELD, \"MEAN\"]) as cur:\n        for zid, m in cur:\n            out[int(zid)] = float(m)\n    return out\n\n# Pixel counts via SUM on 0/1 masks\nvalid_counts = {z:int(round(v)) for z,v in zonal_sum_to_gdb(valid01, \"ZS_valid01\").items()}\nwarm_counts  = {z:int(round(v)) for z,v in zonal_sum_to_gdb(warm01,  \"ZS_warm01\").items()}\ncool_counts  = {z:int(round(v)) for z,v in zonal_sum_to_gdb(cool01,  \"ZS_cool01\").items()}\n\n# Sums of raster values (positive-only and negative-only)\nwarm_sum = zonal_sum_to_gdb(warm_vals, \"ZS_warm_sum\")  # positive sums\ncool_sum = zonal_sum_to_gdb(cool_vals, \"ZS_cool_sum\")  # negative sums\n\n# Means of Net (over valid pixels only)\nnet_mean = zonal_mean_to_gdb(net, \"ZS_net_mean\")\n\n# ===================== 3) WRITE PER-ZONE CSV =====================\nwith open(OUT_CSV, \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\n        \"ZoneID\",\"Ecozone\",\"Total_Pixels\",\n        \"Warm_Pixels\",\"Warm_%\",\"Cool_Pixels\",\"Cool_%\",\n        \"Valid_Pixels\",\"Valid_%\",\"NoData_Pixels\",\"NoData_%\",\n        \"Sum_Warming_Wm2yr\",\"Sum_Cooling_Wm2yr\",\"Net_Mean_Wm2yr\"\n    ])\n    for zid in sorted(total_pixels.keys()):\n        tot  = total_pixels.get(zid, 0)\n        warm = warm_counts.get(zid, 0)\n        cool = cool_counts.get(zid, 0)\n        val  = valid_counts.get(zid, 0)\n        nd   = max(tot - val, 0)\n\n        if tot > 0:\n            warm_pct  = 100.0 * warm / tot\n            cool_pct  = 100.0 * cool / tot\n            valid_pct = 100.0 * val  / tot\n            nodata_pct= 100.0 - valid_pct\n        else:\n            warm_pct = cool_pct = valid_pct = nodata_pct = 0.0\n\n        w.writerow([\n            zid, eco_name.get(zid, str(zid)), tot,\n            warm, round(warm_pct,3), cool, round(cool_pct,3),\n            val, round(valid_pct,3), nd, round(nodata_pct,3),\n            round(warm_sum.get(zid, 0.0), 6),\n            round(cool_sum.get(zid, 0.0), 6),\n            round(net_mean.get(zid, float(\"nan\")), 6)\n        ])\n\nprint(\"Per-zone summary written:\", OUT_CSV)\n\n# ===================== 4) WRITE GRAND TOTALS CSV =====================\ngrand_warm_sum = sum(warm_sum.values())\ngrand_cool_sum = sum(cool_sum.values())\nwith open(OUT_TOTALS, \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"Metric\",\"Value\",\"Units\"])\n    w.writerow([\"Total_warming_sum\", round(grand_warm_sum, 6), \"W m^-2 yr^-1 (sum of pixel values >0)\"])\n    w.writerow([\"Total_cooling_sum\", round(grand_cool_sum, 6), \"W m^-2 yr^-1 (sum of pixel values <0)\"])\n\nprint(\"Grand totals written:\", OUT_TOTALS)\n\n# ===================== (OPTIONAL) AREA-INTEGRATED TOTALS =====================\n","metadata":{"id":"HTMgvnjqixUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2wMxH5AhvwtF","jupyter":{"source_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"kasYryJNvwRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#====TO GET AREA-INTEGRATED TOTALS WE IMPLEMENTED THE STEPS BELOW =====","metadata":{"id":"2l9CPZfVcHPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import arcpy, os, csv\nfrom arcpy.sa import Raster, IsNull, Con, ZonalStatisticsAsTable\narcpy.CheckOutExtension(\"Spatial\")\n\n# --- EDIT THESE WHEN YOU WANT TO SPECIFY PATHS  ---\nGDB         = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\ENERGY_FLUXES_NEWCOLLECTIONS.gdb\"\nZONE_RAS    = \"Study_tse_Raster\"   # integer zone raster (1..9)\nZONE_FIELD  = \"Value\"              # 'Value' or 'VALUE'\nNET_RASTER  = \"Net_exchange2\"\nOUT_CSV     = r\"X:\\ENERGY_FLUXES_NEWCOLLECTIONS\\net2_zone_percentages.csv\"\n\neco_name = {1:\"SA\",2:\"TP\",3:\"BS\",4:\"BC\",5:\"TC\",6:\"HP\",7:\"TSW\",8:\"TCO\",9:\"TSE\"}\n\narcpy.env.workspace       = GDB\narcpy.env.overwriteOutput = True\narcpy.env.snapRaster      = ZONE_RAS\narcpy.env.extent          = arcpy.Describe(ZONE_RAS).extent\narcpy.env.cellSize        = arcpy.Describe(ZONE_RAS).meanCellWidth\n\n# Ensure RAT exists & get total pixel counts per zone\narcpy.management.BuildRasterAttributeTable(ZONE_RAS, \"OVERWRITE\")\nfields = [f.name for f in arcpy.ListFields(ZONE_RAS)]\nval_f  = ZONE_FIELD if ZONE_FIELD in fields else next(f for f in fields if f.lower()==\"value\")\ncnt_f  = \"COUNT\" if \"COUNT\" in fields else next(f for f in fields if f.lower()==\"count\")\n\ntotal_pixels = {}\nwith arcpy.da.SearchCursor(ZONE_RAS, [val_f, cnt_f]) as cur:\n    for zid, cnt in cur:\n        total_pixels[int(zid)] = int(cnt or 0)\n\n# Masks from Net_exchange2\nnet = Raster(NET_RASTER)\nvalid01 = Con(IsNull(net), 0, 1)\nwarm01  = Con(net > 0, 1, 0)\ncool01  = Con(net < 0, 1, 0)\n\ndef zonal_sum_to_gdb(mask, name):\n    \"\"\"Write ZonalStatisticsAsTable output to the GDB (not in_memory) and read SUM.\"\"\"\n    tbl = os.path.join(GDB, name)\n    if arcpy.Exists(tbl):\n        arcpy.Delete_management(tbl)\n    ZonalStatisticsAsTable(ZONE_RAS, val_f, mask, tbl, \"DATA\", \"SUM\")\n    # sanity: if tool produced no table or no rows, raise with messages\n    if not arcpy.Exists(tbl):\n        raise RuntimeError(f\"ZonalStatisticsAsTable failed for {name}:\\n{arcpy.GetMessages(2)}\")\n    # find SUM field reliably\n    sum_field = None\n    for f in arcpy.ListFields(tbl):\n        if f.name.upper() == \"SUM\":\n            sum_field = f.name; break\n    if sum_field is None:\n        for f in arcpy.ListFields(tbl):\n            if f.name.upper().endswith(\"_SUM\") or f.name.upper()==\"SUM_\":\n                sum_field = f.name; break\n    if sum_field is None:\n        raise RuntimeError(f\"SUM field not found in {tbl}. Fields: {[f.name for f in arcpy.ListFields(tbl)]}\")\n    # read counts\n    out = {}\n    with arcpy.da.SearchCursor(tbl, [val_f, sum_field]) as cur:\n        for zid, s in cur:\n            out[int(zid)] = int(round(s or 0))\n    return out\n\nvalid_counts = zonal_sum_to_gdb(valid01, \"ZS_valid01\")\nwarm_counts  = zonal_sum_to_gdb(warm01,  \"ZS_warm01\")\ncool_counts  = zonal_sum_to_gdb(cool01,  \"ZS_cool01\")\n\n# Zonal MEAN of Net (over DATA only), also to GDB\nzs_mean_tbl = os.path.join(GDB, \"ZS_net_mean\")\nif arcpy.Exists(zs_mean_tbl):\n    arcpy.Delete_management(zs_mean_tbl)\nZonalStatisticsAsTable(ZONE_RAS, val_f, NET_RASTER, zs_mean_tbl, \"DATA\", \"MEAN\")\n\nmean_by_zone = {}\nwith arcpy.da.SearchCursor(zs_mean_tbl, [val_f, \"MEAN\"]) as cur:\n    for zid, m in cur:\n        mean_by_zone[int(zid)] = float(m)\n\n# Assemble & write CSV\nwith open(OUT_CSV, \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"ZoneID\",\"Ecozone\",\"Total_Pixels\",\n                \"Valid_Pixels\",\"Valid_%\",\"NoData_Pixels\",\"NoData_%\",\n                \"Warm_Pixels\",\"Warm_%\",\"Cool_Pixels\",\"Cool_%\",\n                \"Net_Mean_Wm2yr\"])\n    for zid in sorted(total_pixels.keys()):\n        tot  = total_pixels.get(zid, 0)\n        val  = valid_counts.get(zid, 0)\n        nd   = max(tot - val, 0)\n        warm = warm_counts.get(zid, 0)\n        cool = cool_counts.get(zid, 0)\n        if tot > 0:\n            valid_pct = 100.0 * val  / tot\n            nodata_pct= 100.0 * nd   / tot\n            warm_pct  = 100.0 * warm / tot\n            cool_pct  = 100.0 * cool / tot\n        else:\n            valid_pct = nodata_pct = warm_pct = cool_pct = 0.0\n        w.writerow([zid, eco_name.get(zid, str(zid)), tot,\n                    val, round(valid_pct,3), nd, round(nodata_pct,3),\n                    warm, round(warm_pct,3), cool, round(cool_pct,3),\n                    round(mean_by_zone.get(zid, float(\"nan\")), 6)])\nprint(\"Done:\", OUT_CSV)\n","metadata":{"id":"pixs0YhcixQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Codes for Ecozone level correlation between NDVI trend and temperature trend \r\n#Codes for Ecozone level correlation between NDVI trend and Net-Annual energy trend fluxes, \r\n#From SECTION A2, we computed the correlation between NDVI trend and Net-Annual energy trend fluxes\r\n#From SECTION c2, we computed the Global level comparison: Welch t-test (unequal variances) & Mannâ€“Whitney U (non-parametric).\r\n","metadata":{"id":"5kUFBWG1lN-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SECTION A2: Here we present the correlation code we used to globally correlate temperature trend versus the cooling and warming energy trend fluxes\r\n#This particular task was implemented directly in the Google Earth Engine\r\n#HERE we presnetd Pipeline for cooling energy flux trend  versus temperature\r\n#BUT TO CORRELATE FOR WARMING TREND, SWITCHING COOLING_total to WARMING_total will get the results for temperature trend versus the warming energy trend fluxes\r\n#See README file for information on these grids and proper download for verifications\r\n\r\n// ===========================================================\r\n// Mean T2m (Â°C), 1986â€“2023 â€” clipped to study area & aligned to Cooling grid\r\n// ===========================================================\r\n\r\n// ---- INPUTS ----\r\nvar ZONES_ASSET = 'projects/danielsiglab/assets/study_area_tse_tw';\r\nvar COOL_ASSET  = 'projects/danielsiglab/assets/Cooling_total'; #BUT TO CORRELATE FOR WARMING TREND, SWITCH COOLING_total to WARMING_total \r\nvar START_DATE  = '1986-01-01';\r\nvar END_DATE    = '1990-12-31';\r\n\r\n// ---- LOADS ----\r\nvar zones   = ee.FeatureCollection(ZONES_ASSET);\r\nvar region  = zones.geometry();                   // <-- no buffer(0)\r\nvar cooling = ee.Image(COOL_ASSET).select(0);\r\n\r\nvar coolProj = cooling.projection();\r\nvar SCALE    = coolProj.nominalScale();\r\n\r\n// ERA5-Land monthly temperatures\r\nvar era5 = ee.ImageCollection('ECMWF/ERA5_LAND/MONTHLY')\r\n  .filterDate(START_DATE, END_DATE);\r\n\r\n// ---- BUILD MULTI-YEAR MEAN TEMPERATURE (Â°C) ----\r\nvar Tmean_C = era5\r\n  .select('temperature_2m')        // Kelvin\r\n  .mean()                          // mean of all months 1986â€“2023 (NoData ignored)\r\n  .subtract(273.15)                // -> Â°C\r\n  .rename('Tmean_1986_2023_C')\r\n  .clip(zones);                    // clip to study area polygons\r\n\r\n// ---- PREVIEW ----\r\nMap.centerObject(zones, 4);\r\nMap.addLayer(\r\n  Tmean_C,\r\n  {min: -30, max: 30, palette: ['#313695','#74add1','#ffffbf','#f46d43','#a50026']},\r\n  'Mean T (Â°C) 1986â€“2023 â€” clipped'\r\n);\r\n\r\n// ---- EXPORT TO ASSET (aligned to Cooling_total grid) ----\r\nvar OUT_ASSET_ID = 'projects/danielsiglab/assets/TmeanC_1986_2023_studyarea_coolingGrid';\r\n\r\nExport.image.toAsset({\r\n  image: Tmean_C,\r\n  description: 'Export_TmeanC_1986_2023_studyarea_coolingGrid',\r\n  assetId: OUT_ASSET_ID,\r\n  region: region,      // clipped extent\r\n  crs: coolProj,       // align projection to Cooling_total\r\n  scale: SCALE,        // align resolution to Cooling_total\r\n  maxPixels: 1e13\r\n});\r\n\r\n\r\n// ==== CORRELATION: Cooling_total (X) vs mean T (Y) per ecozone (Name) ====\r\n\r\n// ==== CORRELATION: Cooling_total (X) vs mean T (Y) per ecozone (Name) ====\r\n\r\n// ==== CORRELATION: Cooling_total (X) vs mean T (Y) per ecozone (Name) ====\r\n\r\n// ==== CORRELATION: Cooling_total (X) vs mean T (Y) per ecozone (Name) ====\r\n\r\n// 1) Align mean temperature to Cooling_total grid and scale\r\nvar Tmean_aligned = Tmean_C\r\n  .rename('Y')\r\n  .resample('bilinear')\r\n  .reproject({crs: coolProj, scale: SCALE});  // exact pixel alignment\r\n\r\n// 2) Pair the bands and co-mask\r\nvar pair = ee.Image.cat(\r\n  cooling.rename('X'),\r\n  Tmean_aligned\r\n).updateMask(cooling.mask().and(Tmean_aligned.mask()));\r\n\r\n// helper: safely pull rho from any reducer key\r\nfunction safeGetRho(dict) {\r\n  dict = ee.Dictionary(dict);\r\n  var keys = ee.List(dict.keys());\r\n  // Prefer 'spearmansCorrelation', else 'correlation', else first value if any\r\n  return ee.Algorithms.If(\r\n    keys.contains('spearmansCorrelation'), dict.get('spearmansCorrelation'),\r\n    ee.Algorithms.If(\r\n      keys.contains('correlation'), dict.get('correlation'),\r\n      ee.Algorithms.If(keys.size().gt(0), dict.values().get(0), null)\r\n    )\r\n  );\r\n}\r\n\r\n// 3) Spearman Ï per polygon\r\nvar perZone = zones.map(function (f) {\r\n  // buffer by half a pixel so boundary pixels are counted\r\n  var geom = f.geometry().buffer(ee.Number(SCALE).divide(2));\r\n\r\n  // robust pair count: sum of ones over the pair mask\r\n  var n = ee.Number(\r\n    ee.Image.constant(1).rename('ones')\r\n      .updateMask(pair.select('X').mask())           // co-mask already enforced\r\n      .reduceRegion({\r\n        reducer: ee.Reducer.sum(),\r\n        geometry: geom,\r\n        scale: SCALE,\r\n        maxPixels: 1e13,\r\n        bestEffort: true\r\n      })\r\n      .get('ones')\r\n  );\r\n\r\n  // compute spearman only if n >= 2\r\n  var rho = ee.Algorithms.If(\r\n    n.gte(2),\r\n    safeGetRho(\r\n      pair.reduceRegion({\r\n        reducer: ee.Reducer.spearmansCorrelation(),\r\n        geometry: geom,\r\n        scale: SCALE,\r\n        maxPixels: 1e13,\r\n        bestEffort: true\r\n      })\r\n    ),\r\n    null\r\n  );\r\n\r\n  return ee.Feature(null, {\r\n    zone_name: ee.String(f.get('Name')),\r\n    n: n,\r\n    rho: rho,\r\n    note: ee.String(ee.Algorithms.If(n.gte(2), 'ok', 'insufficient_n'))\r\n  });\r\n});\r\n\r\n// 4) Preview the actual table (be sure you're printing perZone, not zones)\r\nprint('Spearman Ï per ecozone (Cooling_total vs mean T 1986â€“2023):', perZone.limit(9));\r\nprint('First row values:', ee.Feature(perZone.first()).toDictionary(['zone_name','n','rho','note']));\r\n\r\n\r\n// 5) EXPORT table to Drive (as before)\r\nExport.table.toDrive({\r\n  collection: perZone,\r\n  description: 'Cooling_vs_Tmean_1986_2023_Spearman_perZoneName',\r\n  folder: 'Cooling_Total_Correlation',\r\n  fileFormat: 'CSV'\r\n});\r\n\r\n","metadata":{"id":"ES98HhhxlN5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#SECTION B2: Codes for Ecozone level correlation between NDVI trend and Net-Annual energy trend fluxes\n#we implemented this in ARGISPro python-API extension\n\n\n\nimport arcpy\nfrom arcpy.sa import ExtractMultiValuesToPoints\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import kendalltau   # Kendall Ï„\n\narcpy.env.overwriteOutput = True\n\n# ------------------------------------------------------------------\n# 0. PATHS â€“ Specicy path of your Geodatabase\n# ------------------------------------------------------------------\ngdb = r\"U:\\slope_sumarry\\slope_sumarry.gdb\" #path to the ARGISPro geodatabase(gdb)\narcpy.env.workspace = gdb\n\n# polygon ecozone / study feature class:\necozone_fc = \"Study_tse\"          # this is stduy are shapefile\n\n# rasters (outside the gdb, in the same folder as before)\nndvi_ras = r\"X:\\Aggregated_ndvitrend.tif\"  #This is the NDVI trend grid\nnet_ras  = r\"X:\\Annual_net_FLUX_TREND.tif\" ##This is the net trend energy flux trend grid\n\n# field in Study_tse we will use as ecozone label:\necozone_name_field = \"Name\"     #The is the shapefile of ecozones\n\n# how many random points to generate\nnumber_of_points = 54000 #here the number can be varied for each separate runs\n\n# ------------------------------------------------------------------\n# 1. CHECK THAT INPUTS EXIST\n# ------------------------------------------------------------------\nprint(\"Workspace:\", arcpy.env.workspace)\nprint(\"Feature classes in GDB:\")\nfor fc in arcpy.ListFeatureClasses():\n    print(\"  -\", fc)\n\nif not arcpy.Exists(ecozone_fc):\n    raise RuntimeError(f\"Feature class '{ecozone_fc}' not found in {gdb}\")\n\nif not arcpy.Exists(ndvi_ras):\n    raise RuntimeError(f\"NDVI raster not found: {ndvi_ras}\")\n\nif not arcpy.Exists(net_ras):\n    raise RuntimeError(f\"NET raster not found:  {net_ras}\")\n\n# ------------------------------------------------------------------\n# 2. CREATE RANDOM POINTS INSIDE Study_tse\n# ------------------------------------------------------------------\nsample_points_fc = gdb + r\"\\eco_rand_pts\"\n\nprint(\"\\nCreating random pointsâ€¦\")\n\n# IMPORTANT FIX: create_multipoint_output must be POINT or MULTIPOINT\narcpy.management.CreateRandomPoints(\n    out_path=gdb,\n    out_name=\"eco_rand_pts\",\n    constraining_feature_class=ecozone_fc,\n    constraining_extent=\"\",               # use polygon extent\n    number_of_points_or_field=number_of_points,\n    minimum_allowed_distance=\"0 Meters\",\n    create_multipoint_output=\"POINT\",    # <-- FIXED\n    multipoint_size=\"\"\n)\n\nprint(\"Random points created:\", sample_points_fc)\n\n# ------------------------------------------------------------------\n# 3. SPATIAL JOIN: ADD ECOZONE Name TO EACH POINT\n# ------------------------------------------------------------------\njoined_points_fc = gdb + r\"\\eco_rand_pts_join\"\n\nprint(\"\\nRunning spatial join to attach ecozone Nameâ€¦\")\n\narcpy.analysis.SpatialJoin(\n    target_features=sample_points_fc,\n    join_features=ecozone_fc,\n    out_feature_class=joined_points_fc,\n    join_operation=\"JOIN_ONE_TO_ONE\",\n    join_type=\"KEEP_COMMON\",\n    match_option=\"INTERSECT\"\n)\n\nprint(\"Spatial join done:\", joined_points_fc)\n\n# ------------------------------------------------------------------\n# 4. EXTRACT NDVI & NET RASTER VALUES TO THE POINTS\n# ------------------------------------------------------------------\nprint(\"\\nChecking out Spatial Analyst and extracting raster valuesâ€¦\")\narcpy.CheckOutExtension(\"Spatial\")\n\nExtractMultiValuesToPoints(\n    in_point_features=joined_points_fc,\n    in_rasters=[[ndvi_ras, \"NDVI_trend\"],\n                [net_ras,  \"NET_trend\"]],\n    bilinear_interpolate_values=\"NONE\"\n)\n\nprint(\"Raster values extracted into fields NDVI_trend and NET_trend.\")\n\n# ------------------------------------------------------------------\n# 5. CONVERT TO PANDAS DATAFRAME\n# ------------------------------------------------------------------\nfields = [ecozone_name_field, \"NDVI_trend\", \"NET_trend\"]\nrecords = []\n\nwith arcpy.da.SearchCursor(joined_points_fc, fields) as cursor:\n    for row in cursor:\n        records.append(row)\n\ndf = pd.DataFrame(records, columns=[\"eco_name\", \"NDVI\", \"NET\"])\n\n# Handle NoData (if -9999 is used) and drop NaNs\ndf = df.replace({-9999: np.nan})\ndf = df.dropna(subset=[\"NDVI\", \"NET\"])\n\nprint(\"\\nTotal valid points after removing NoData:\", len(df))\n\n# ------------------------------------------------------------------\n# 6. KENDALL TAU PER ECOZONE WITH NDVI THRESHOLDS\n# ------------------------------------------------------------------\nndvi_thresholds = [0.0, 0.0015, 0.004, 0.005, 0.006] #Here threholds of the ndvi trend can be specified\n\nresults = []\n\nfor eco, sub in df.groupby(\"eco_name\"):\n    for thr in ndvi_thresholds:\n        # keep points with NDVI trend >= threshold\n        sub_thr = sub[sub[\"NDVI\"] >= thr]\n        n = len(sub_thr)\n        if n < 30:\n            continue  # skip tiny samples\n\n        tau, p = kendalltau(sub_thr[\"NDVI\"], sub_thr[\"NET\"])\n        if np.isnan(tau):\n            continue\n\n        results.append({\n            \"eco_name\": eco,\n            \"NDVI_min\": thr,\n            \"N_points\": n,\n            \"tau\": float(tau),\n            \"p_value\": float(p)\n        })\n\nres_df = pd.DataFrame(results)\nres_df = res_df.sort_values(by=[\"eco_name\", \"NDVI_min\"])\n\nprint(\"\\nKendall tau per ecozone and NDVI-threshold:\")\nprint(res_df.to_string(index=False))\n\n# ------------------------------------------------------------------\n# 7. OPTIONAL: EXPORT RESULTS TO CSV\n# ------------------------------------------------------------------\nout_csv = r\"U:\\slope_summary15\\slope_summary\\NDVI_NET_Kendall_per_Study_tse.csv\"\nres_df.to_csv(out_csv, index=False)\nprint(\"\\nResults written to:\", out_csv)\n\n\n\n\n\n#SECTION C2: Codes for Global level comparison: Welch t-test (unequal variances) & Mannâ€“Whitney U (non-parametric)\n# Here we tested Is the NDVIâ€“energy trend relationship and the energy trend itself different \n#between stable vegetation vs greening-transition pixels\n\n\nimport arcpy\nfrom arcpy.sa import ExtractMultiValuesToPoints\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import kendalltau, ttest_ind, mannwhitneyu\nimport datetime\nimport os\n\narcpy.env.overwriteOutput = False  # To avoid overwrite existing outputs\n\n# ----------------------------------------------------------\n# 0. PATHS / INPUTS\n# ----------------------------------------------------------\n# Geodatabase with COMBINEDVCGL (transition / stable codes)\ngdb_transition = r\"U:\\class_transition\\landcover_transition\\landcover_transition.gdb\"\n\n# Geodatabase with NDVI trend (LINEARSLOPE_MOSAIC) and where we write points\ngdb_slope = r\"U:\\slope_summary15\\slope_sumarry\\slope_sumarry.gdb\"\n\n# 30 m NDVI trend raster (in slope GDB)\nndvi_ras_path = os.path.join(gdb_slope, \"LINEARSLOPE_MOSAIC\")\n\n# 30 m NET energy trend raster (resampled) â€“ in separate GDB\nnet_ras_path = r\"X:\\ndvi_trend_enegry_correlations\\ndvi_trend_enegry_correlations.gdb\\Annual_net_FLUX_TRE_Resample12\"\n\n# Where to store CSV outputs\nout_folder = r\"U:\\class_transition\\landcover_transition\"\n\n# How many UNIQUE 30 m cells to sample (upper bound, no double sampling)\ntotal_unique_cells_to_sample = 000  # adjust if you want more/less\n\n# STABLE CODES (diagonals, including 505)\nstable_codes = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1010, 1111, 1212] #These codes including the greening codes are to be drawn from the transition grid \"COMBINEDVCGL\" which store categorical values of vegetation type transitions\n\n# GREENING TRANSITION CODES (exact list you gave)\ngreening_codes = [\n    106, 107, 108, 109, 110, 111, 112,\n    206, 207, 208, 209, 210, 211, 212,\n    306, 307, 308, 309, 310, 311, 312,\n    405, 406, 407, 408, 409, 410, 411, 412,\n    506, 507, 508, 509, 510, 511, 512,\n    610, 611, 612,\n    910, 911, 912\n]\n\n# Optional max per group after filtering (set to None to keep all)\nmax_per_group = 10000  # e.g. 20000 stable + 20000 greening; set to None to keep all. For each run user can vary it carefully based on available memory.\n\n# Timestamp for unique names\nts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# ----------------------------------------------------------\n# 1. FIND COMBINEDVCGL INSIDE THE TRANSITION GDB\n# ----------------------------------------------------------\nprint(\"Checking COMBINEDVCGL in transition geodatabase...\")\narcpy.env.workspace = gdb_transition\n\nrasters = arcpy.ListRasters()\nprint(\"Rasters found in\", gdb_transition, \":\")\nif rasters:\n    for r in rasters:\n        print(\"  -\", r)\nelse:\n    print(\"  (no rasters listed)\")\n\nif \"COMBINEDVCGL\" not in rasters:\n    raise RuntimeError(\"COMBINEDVCGL not found in landcover_transition.gdb. \"\n                       \"Check the exact raster name in ArcGIS Pro Contents.\")\n\ncombined_ras_name = \"COMBINEDVCGL\" #this grid contains our landcover type transition values\ncombined_ras_path = os.path.join(gdb_transition, combined_ras_name)\ncombined_ras = arcpy.Raster(combined_ras_path)\n\n# ----------------------------------------------------------\n# 2. CHECK NDVI & NET INPUTS\n# ----------------------------------------------------------\nprint(\"\\nChecking NDVI and NET rasters...\")\n\nif not arcpy.Exists(ndvi_ras_path):\n    raise RuntimeError(f\"NDVI trend (LINEARSLOPE_MOSAIC) not found: {ndvi_ras_path}\")\nif not arcpy.Exists(net_ras_path):\n    raise RuntimeError(f\"NET trend (Annual_net_FLUX_TREND) not found: {net_ras_path}\")\n\nndvi_ras = arcpy.Raster(ndvi_ras_path)\nnet_ras  = arcpy.Raster(net_ras_path)\n\n# Make slope GDB the workspace for point outputs\narcpy.env.workspace = gdb_slope\n\n# Sanity print cell sizes\nprint(\"COMBINEDVCGL cell size:\", combined_ras.meanCellWidth, combined_ras.meanCellHeight)\nprint(\"NDVI cell size:\", ndvi_ras.meanCellWidth, ndvi_ras.meanCellHeight)\nprint(\"NET cell size:\", net_ras.meanCellWidth, net_ras.meanCellHeight)\n\narcpy.CheckOutExtension(\"Spatial\")\n\n# ----------------------------------------------------------\n# 3. UNIQUE CELL SAMPLING (NO DOUBLE SAMPLING)\n# ----------------------------------------------------------\nprint(\"\\nSampling unique 30 m cells from COMBINEDVCGL grid...\")\n\nwidth  = combined_ras.width   # number of columns\nheight = combined_ras.height  # number of rows\nn_total_cells = width * height\nprint(f\"Raster grid: width={width}, height={height}, total cells={n_total_cells}\")\n\nn_sample = min(total_unique_cells_to_sample, n_total_cells)\nprint(f\"Sampling {n_sample} UNIQUE cells (no replacement).\")\n\nrng = np.random.default_rng()\nlin_idx = rng.choice(n_total_cells, size=n_sample, replace=False)\n\nrows = lin_idx // width\ncols = lin_idx % width\nsampled_rc = np.column_stack((rows, cols))  # (n_sample, 2) [row, col]\n\n# Convert row/col -> XY (cell centers)\ndesc = arcpy.Describe(combined_ras)\next = desc.extent\ncellsize_x = combined_ras.meanCellWidth\ncellsize_y = combined_ras.meanCellHeight\n\nx_min = ext.XMin\ny_max = ext.YMax\n\n# Create point FC for these unique sampled cells (in slope GDB)\npts_name = f\"VCGL_uniqueCells_pts_{ts}\"\npts_fc = os.path.join(gdb_slope, pts_name)\n\nprint(\"Creating point feature class:\", pts_fc)\n\nspatial_ref = combined_ras.spatialReference\narcpy.management.CreateFeatureclass(\n    out_path=gdb_slope,\n    out_name=pts_name,\n    geometry_type=\"POINT\",\n    spatial_reference=spatial_ref\n)\n\n# Insert points (geometry only, no cell_id)\nwith arcpy.da.InsertCursor(pts_fc, [\"SHAPE@XY\"]) as icur:\n    for (row, col) in sampled_rc:\n        x = x_min + (col + 0.5) * cellsize_x\n        y = y_max - (row + 0.5) * cellsize_y\n        icur.insertRow([(x, y)])\n\nprint(\"Unique-cell points created:\", pts_fc)\n\n# ----------------------------------------------------------\n# 4. EXTRACT COMBINEDVCGL, NDVI, NET TO THESE POINTS\n# ----------------------------------------------------------\nprint(\"\\nExtracting COMBINEDVCGL, NDVI_trend, NET_trend to unique-cell points...\")\n\nExtractMultiValuesToPoints(\n    in_point_features=pts_fc,\n    in_rasters=[\n        [combined_ras_path, \"VCGL_val\"],\n        [ndvi_ras_path,     \"NDVI_trend\"],\n        [net_ras_path,      \"NET_trend\"]\n    ],\n    bilinear_interpolate_values=\"NONE\"\n)\n\nprint(\"Fields VCGL_val, NDVI_trend, NET_trend added to:\", pts_fc)\n\n# ----------------------------------------------------------\n# 5. BUILD DATAFRAME AND FILTER\n# ----------------------------------------------------------\nprint(\"\\nBuilding DataFrame and filtering valid samples...\")\n\nfields = [\"VCGL_val\", \"NDVI_trend\", \"NET_trend\", \"SHAPE@XY\"]\nrecords = []\nwith arcpy.da.SearchCursor(pts_fc, fields) as cur:\n    for vcgl, ndvi_val, net_val, (x, y) in cur:\n        records.append((vcgl, ndvi_val, net_val, x, y))\n\ndf = pd.DataFrame(records, columns=[\"VCGL_val\", \"NDVI_trend\", \"NET_trend\", \"X\", \"Y\"])\n\n# Clean NoData if present (e.g. -9999) and drop NaNs\ndf = df.replace({-9999: np.nan})\ndf = df.dropna(subset=[\"VCGL_val\", \"NDVI_trend\", \"NET_trend\"])\n\n# Remove background / non-vegetation if coded as 0\ndf = df[df[\"VCGL_val\"] != 0]\n\nprint(\"Total valid unique-cell samples after filtering:\", len(df))\n\n# ----------------------------------------------------------\n# 6. SPLIT INTO STABLE vs GREENING\n# ----------------------------------------------------------\ndf_stable = df[df[\"VCGL_val\"].isin(stable_codes)].copy()\ndf_green  = df[df[\"VCGL_val\"].isin(greening_codes)].copy()\n\nprint(\"  STABLE samples (before downsampling):   \", len(df_stable))\nprint(\"  GREENING samples (before downsampling): \", len(df_green))\n\nif len(df_stable) < 10:\n    raise RuntimeError(\"Too few STABLE samples after filtering; increase total_unique_cells_to_sample.\")\nif len(df_green) < 10:\n    raise RuntimeError(\"Too few GREENING samples after filtering; increase total_unique_cells_to_sample.\")\n\n# Optional: downsample to limit per group\ndef downsample_group(df_group, n_max):\n    if (n_max is None) or (len(df_group) <= n_max):\n        return df_group\n    return df_group.sample(n=n_max, random_state=42)\n\ndf_stable = downsample_group(df_stable, max_per_group)\ndf_green  = downsample_group(df_green, max_per_group)\n\nprint(\"After optional downsampling:\")\nprint(\"  STABLE samples:   \", len(df_stable))\nprint(\"  GREENING samples: \", len(df_green))\n\n# ----------------------------------------------------------\n# 7. KENDALL Ï„: NDVI vs NET FOR EACH GROUP\n# ----------------------------------------------------------\nprint(\"\\nComputing Kendall Ï„ between NDVI_trend and NET_trend...\")\n\ntau_stable, p_stable = kendalltau(df_stable[\"NDVI_trend\"], df_stable[\"NET_trend\"])\ntau_green,  p_green  = kendalltau(df_green[\"NDVI_trend\"],  df_green[\"NET_trend\"])\n\nprint(\"\\nKendall Ï„ results (NDVI_trend vs NET_trend):\")\nprint(f\"  STABLE group:   tau = {tau_stable:.4f}, p = {p_stable:.3e}, N = {len(df_stable)}\")\nprint(f\"  GREENING group: tau = {tau_green:.4f}, p = {p_green:.3e}, N = {len(df_green)}\")\n\n# ----------------------------------------------------------\n# 8. COMPARE NET_TREND BETWEEN GROUPS (t-test & Mannâ€“Whitney)\n# ----------------------------------------------------------\nprint(\"\\nComparing NET_trend distributions (GREENING vs STABLE)...\")\n\nnet_stable = df_stable[\"NET_trend\"].values\nnet_green  = df_green[\"NET_trend\"].values\n\n# Welch t-test (unequal variances)\nt_stat, p_t = ttest_ind(net_green, net_stable, equal_var=False, nan_policy=\"omit\")\n\n# Mannâ€“Whitney U (non-parametric)\nu_stat, p_u = mannwhitneyu(net_green, net_stable, alternative=\"two-sided\")\n\nprint(\"Two-sample t-test (NET_trend, GREENING vs STABLE):\")\nprint(f\"  t = {t_stat:.4f}, p = {p_t:.3e}\")\n\nprint(\"Mannâ€“Whitney U test (NET_trend, GREENING vs STABLE):\")\nprint(f\"  U = {u_stat:.4f}, p = {p_u:.3e}\")\n\n# ----------------------------------------------------------\n# 9. EXPORT CSVs\n# ----------------------------------------------------------\nprint(\"\\nExporting CSVs...\")\n\ncsv_stable = os.path.join(out_folder, f\"StableVCGL_NDVI_NET_uniqueCells_{ts}.csv\")\ncsv_green  = os.path.join(out_folder, f\"GreeningVCGL_NDVI_NET_uniqueCells_{ts}.csv\")\ncsv_both   = os.path.join(out_folder, f\"Stable_vs_Greening_NDVI_NET_uniqueCells_{ts}.csv\")\n\ndf_stable.to_csv(csv_stable, index=False)\ndf_green.to_csv(csv_green, index=False)\n\ndf_all = pd.concat([\n    df_stable.assign(group=\"stable\"),\n    df_green.assign(group=\"greening\")\n], ignore_index=True)\ndf_all.to_csv(csv_both, index=False)\n\nprint(\"CSV outputs:\")\nprint(\"  STABLE:   \", csv_stable)\nprint(\"  GREENING: \", csv_green)\nprint(\"  BOTH:     \", csv_both)\n\nprint(\"\\nDone.\")\n\n\n\n\n\n\n","metadata":{"id":"weuPgpsXmdbc"},"execution_count":null,"outputs":[]}]}